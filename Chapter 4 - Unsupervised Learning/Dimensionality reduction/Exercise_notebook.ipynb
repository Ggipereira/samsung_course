{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Vm8oebzAT6"
      },
      "source": [
        "# RiceGraininator 5000 Deluxe\n",
        "## Introduction\n",
        "\n",
        "Congratulations, aspiring data scientists! You’ve been recruited by none other than Dr. Ingrain Doofennutrientz, the legendary (and slightly misunderstood) inventor, to help bring his latest masterpiece to life: the **RiceGraininator 5000 Deluxe**! This marvelous machine is designed to solve one of humanity’s most pressing issues—accurately identifying rice varieties.  \n",
        "\n",
        "Why? Because Dr. Doofennutrientz once lost an all-you-can-eat sushi contest after mixing up Arborio and Jasmine rice. Determined to never face such a mix-up again, he vowed to build the ultimate rice-classifying robot. But alas, the **RiceGraininator 5000 Deluxe** is only as good as its algorithms—and that’s where you come in!  \n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "Your task is to train and evaluate the *RiceGraininator 5000 Deluxe* using a carefully curated dataset. This subset consists of **5,000 observations**, with approximately **1,000 grains per rice species**: Jasmine, Basmati, Arborio, Karacadag, and Ipsala. Each grain was meticulously processed to extract **106 features** derived from advanced image processing techniques:  \n",
        "\n",
        "- **12 morphological features**  \n",
        "- **4 shape features** derived from morphological characteristics  \n",
        "- **90 color features** from five different color spaces (RGB, HSV, Lab*, YCbCr, XYZ)  \n",
        "\n",
        "The objective is to first perform a logistic regression to classify the rice varieties. Then, you will apply **Principal Component Analysis (PCA)** to reduce feature complexity and compare the results. Will your algorithm achieve culinary excellence and secure Dr. Doofennutrientz's legacy? Time—and your coding—will tell!  \n",
        "\n",
        "## References\n",
        "1. KOKLU, M., CINAR, I., & TASPINAR, Y. S. (2021). Classification of rice varieties with deep learning methods. *Computers and Electronics in Agriculture, 187,* 106285. DOI: [10.1016/j.compag.2021.106285](https://doi.org/10.1016/j.compag.2021.106285)  \n",
        "2. CINAR, I., & KOKLU, M. (2021). Determination of Effective and Specific Physical Features of Rice Varieties by Computer Vision In Exterior Quality Inspection. *Selcuk Journal of Agriculture and Food Sciences, 35*(3), 229-243. DOI: [10.15316/SJAFS.2021.252](https://doi.org/10.15316/SJAFS.2021.252)  \n",
        "3. CINAR, I., & KOKLU, M. (2022). Identification of Rice Varieties Using Machine Learning Algorithms. *Journal of Agricultural Sciences, 28*(2), 307-325. DOI: [10.15832/ankutbd.862482](https://doi.org/10.15832/ankutbd.862482)  \n",
        "4. CINAR, I., & KOKLU, M. (2019). Classification of Rice Varieties Using Artificial Intelligence Methods. *International Journal of Intelligent Systems and Applications in Engineering, 7*(3), 188-194. DOI: [10.18201/ijisae.2019355381](https://doi.org/10.18201/ijisae.2019355381)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3SPe1ILCzAUB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oQI5dZwTzAUD"
      },
      "outputs": [],
      "source": [
        "# target variable is called CLASS\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/samsung-ai-course/8th-9th-edition/refs/heads/main/Chapter%204%20-%20Unsupervised%20Learning/Dimensionality%20reduction/data/rice.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Bn0ItyTnzAUE",
        "outputId": "10e752f1-61e3-4eb6-dc9f-6ce3f8afc001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.996\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Arborio       1.00      0.99      0.99       203\n",
            "     Basmati       1.00      1.00      1.00       212\n",
            "      Ipsala       1.00      1.00      1.00       198\n",
            "     Jasmine       0.99      0.99      0.99       201\n",
            "   Karacadag       1.00      1.00      1.00       186\n",
            "\n",
            "    accuracy                           1.00      1000\n",
            "   macro avg       1.00      1.00      1.00      1000\n",
            "weighted avg       1.00      1.00      1.00      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'CLASS'  # Adjust this if your target column has a different name\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"logreg\", LogisticRegression(random_state=55, max_iter=2000))\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Lmh2gZK-zAUF",
        "outputId": "d5cefe8b-ff83-4898-e0f4-5cf7c6fb625d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.997"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# do the PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "pca=PCA(n_components=15)\n",
        "X_train_pca=pca.fit_transform(X_train_scaled)\n",
        "X_test_pca=pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "# remeber chose number of components and fit PCA only on train datast\n",
        "# do the transform in both train and test dataset\n",
        "# train again\n",
        "log=LogisticRegression(random_state=5)\n",
        "log.fit(X_train_pca, y_train)\n",
        "y_predi=log.predict(X_test_pca)\n",
        "# Evaluate the results\n",
        "acu=accuracy_score(y_test, y_predi)\n",
        "acu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpHQ6cTIzAUG"
      },
      "source": [
        "# Other approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwOtnMWzzAUH"
      },
      "outputs": [],
      "source": [
        "# Now lets use another model which is not susceptible to the amount of variables like random forest and see the results\n",
        "# So what you conclude from this ? it is possible to use PCA for feature engineering but sometimes it is just easir to use a model that is robust to amount f variables"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}