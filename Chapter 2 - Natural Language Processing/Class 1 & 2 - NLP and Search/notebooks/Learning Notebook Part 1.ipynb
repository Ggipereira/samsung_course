{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCRaykPud76l"
      },
      "source": [
        "# Class 1 & 2: NLP and Search\n",
        "## Learning Notebook Part 1 - Foundation: Preprocessing & Bag of Words\n",
        "\n",
        "**Welcome!** This notebook focuses on **preprocessing and tokenization** - the foundation of all NLP work!\n",
        "\n",
        "**This notebook will:**\n",
        "- üìù Teach text preprocessing with **interactive TODOs** for class participation\n",
        "- üîß Show regex patterns and tokenization techniques step-by-step  \n",
        "- üí° Explain concepts with examples (complete implementations are in Exercise Notebook)\n",
        "\n",
        "**For hands-on practice:**\n",
        "- Complete implementations and exercises are in the **Exercise Notebook**\n",
        "\n",
        "Let's start by understanding why text preprocessing matters!\n",
        "- Convert text into numbers that computers can understand\n",
        "\n",
        "**By the end**: You'll understand text preprocessing, Bag of Words (word counts), and how to convert text to numbers. You'll learn that BOW is **syntactic** (word-based, no meaning) - true semantic search (understanding meaning) comes in Class 3 with dense embeddings!\n",
        "\n",
        "**Important**: **Semantic = meaning**. In this class, we learn **syntactic** models (BOW) that work with word counts but don't understand meaning. Semantic models (embeddings) that understand meaning are in Class 3!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXiXqifud76n"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö Useful Resources & Tools\n",
        "\n",
        "Before we dive in, here are essential resources you'll find helpful throughout this notebook:\n",
        "\n",
        "> **üí° Learning Philosophy**: For the sake of **deeper understanding**, we'll implement many things from scratch in this course. This helps you truly grasp how the algorithms work, what the challenges are, and why certain design decisions matter. However, keep in mind that there are excellent tools and libraries available to help you in real-world projects once you understand the fundamentals. We'll use both approaches - building from scratch to learn, and leveraging tools to be productive!\n",
        "\n",
        "### üîß Essential Libraries & Documentation\n",
        "\n",
        "**Python Standard Library:**\n",
        "- **`re` (Regular Expressions)**: [Python `re` documentation](https://docs.python.org/3/library/re.html)\n",
        "  - Built-in regex support - no installation needed!\n",
        "  - Essential for text cleaning and pattern matching\n",
        "\n",
        "**NLP Libraries:**\n",
        "- **NLTK (Natural Language Toolkit)**: [NLTK Documentation](https://www.nltk.org/)\n",
        "  - Tokenization, stop words, stemming, lemmatization\n",
        "  - `pip install nltk`\n",
        "  \n",
        "- **spaCy**: [spaCy Documentation](https://spacy.io/)\n",
        "  - Industrial-strength NLP with built-in normalization\n",
        "  - `pip install spacy` + `python -m spacy download en_core_web_sm`\n",
        "  \n",
        "- **scikit-learn**: [sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\n",
        "  - `CountVectorizer`, `TfidfVectorizer` for text vectorization\n",
        "  - Already included in most data science environments\n",
        "\n",
        "**Text Processing Utilities:**\n",
        "- **Unidecode**: Unicode normalization - `pip install unidecode`\n",
        "- **contractions**: Expand contractions - `pip install contractions`\n",
        "- **TextBlob**: Simple NLP API - `pip install textblob`\n",
        "\n",
        "### üß™ Regex Testing & Learning Tools\n",
        "\n",
        "**Regex101** - Interactive Regex Tester: [https://regex101.com/](https://regex101.com/)\n",
        "- ‚≠ê **Highly Recommended!** Test your regex patterns in real-time\n",
        "- See matches highlighted, understand each part of your pattern\n",
        "- Supports Python regex flavor\n",
        "- Great for debugging complex patterns\n",
        "\n",
        "**Other Regex Resources:**\n",
        "- **Regex Cheat Sheet**: [Quick Reference](https://www.rexegg.com/regex-quickstart.html)\n",
        "- **Python Regex HOWTO**: [Official Python Guide](https://docs.python.org/3/howto/regex.html)\n",
        "- **Regex Crossword**: [Learn by playing!](https://regexcrossword.com/)\n",
        "\n",
        "### üìñ Quick Reference: Regex Cheat Sheet\n",
        "\n",
        "```\n",
        "CHARACTER CLASSES\n",
        ".          Any character except newline\n",
        "\\w         Word character (letter, digit, underscore)\n",
        "\\W         Non-word character\n",
        "\\d         Digit (0-9)\n",
        "\\D         Non-digit\n",
        "\\s         Whitespace (space, tab, newline)\n",
        "\\S         Non-whitespace\n",
        "[abc]      Any of a, b, or c\n",
        "[^abc]     Not a, b, or c\n",
        "[a-z]      Character range\n",
        "\n",
        "ANCHORS\n",
        "^          Start of string\n",
        "$          End of string\n",
        "\\b         Word boundary\n",
        "\n",
        "QUANTIFIERS\n",
        "*          0 or more (greedy)\n",
        "+          1 or more (greedy)\n",
        "?          0 or 1 (optional)\n",
        "{n}        Exactly n times\n",
        "{n,}       n or more times\n",
        "{n,m}      Between n and m times\n",
        "*?         Non-greedy (lazy) match\n",
        "\n",
        "GROUPS & CAPTURES\n",
        "(abc)      Capture group\n",
        "(?:abc)    Non-capturing group\n",
        "(?P<name>abc)  Named group\n",
        "\\1         Backreference to group 1\n",
        "\n",
        "LOOKAROUNDS\n",
        "(?=abc)    Positive lookahead\n",
        "(?!abc)    Negative lookahead\n",
        "(?<=abc)   Positive lookbehind\n",
        "(?<!abc)   Negative lookbehind\n",
        "\n",
        "ESCAPE SEQUENCES\n",
        "\\.         Literal period\n",
        "\\\\         Literal backslash\n",
        "\\n         Newline\n",
        "\\t         Tab\n",
        "```\n",
        "\n",
        "### üí° Pro Tips\n",
        "\n",
        "1. **Start with Regex101**: Test patterns before coding\n",
        "2. **Use raw strings**: `r\"pattern\"` in Python to avoid escaping issues\n",
        "3. **Test incrementally**: Build complex patterns step by step\n",
        "4. **Read the docs**: Python's `re` module has great examples\n",
        "5. **Practice**: Regex is a skill that improves with use!\n",
        "\n",
        "**Remember**: You don't need to memorize everything - bookmark these resources and refer back as needed!\n",
        "\n",
        "---\n",
        "\n",
        "### üéì From Scratch ‚Üí Tools: The Learning Path\n",
        "\n",
        "**In this course, you'll:**\n",
        "1. ‚úÖ **Build from scratch** - Implement tokenization, TF-IDF, similarity calculations manually\n",
        "2. ‚úÖ **Understand the why** - Learn the challenges, edge cases, and design decisions\n",
        "3. ‚úÖ **Then use tools** - Apply libraries like `sklearn`, `spaCy`, `NLTK` with full understanding\n",
        "\n",
        "**Why this approach?**\n",
        "- **Deeper understanding**: You'll know what's happening under the hood\n",
        "- **Better debugging**: When things go wrong, you'll know where to look\n",
        "- **Informed choices**: You'll choose the right tool for the right job\n",
        "- **Customization**: You'll be able to modify and extend tools when needed\n",
        "\n",
        "**In real projects**: Use the tools! But your foundation from building from scratch will make you a better practitioner. üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYOCpTJd76p"
      },
      "source": [
        "---\n",
        "\n",
        "## Today's Goal: Building a Movie Search System\n",
        "\n",
        "**The Problem**: You're building a movie recommendation system. Users want to:\n",
        "- üîç **Search** for movies by description (e.g., \"space adventure\", \"mind-bending thriller\")\n",
        "- üìä **Discover** similar movies automatically\n",
        "- üí° **Understand** meaning, not just match exact words\n",
        "\n",
        "**The Challenge**:\n",
        "- \"Space adventure\" should find \"cosmic journey\" and \"galactic exploration\" (requires understanding meaning - synonyms!)\n",
        "- \"Mind-bending\" should find \"psychological thriller\" and \"complex narrative\" (requires understanding meaning!)\n",
        "- We want to understand **meaning**, not just keywords!\n",
        "\n",
        "**What we'll learn TODAY (Syntactic approaches)**:\n",
        "1. Start with simple keyword search and multiple keyword search (see their limitations)\n",
        "2. Learn simple tokenization (splitting text into words)\n",
        "3. Learn text preprocessing (cleaning, tokenization, regex)\n",
        "4. Learn n-grams (feature extraction - capturing word order)\n",
        "5. Convert text to numbers (Bag of Words - word counts/vectorization)\n",
        "6. **Next in Part 2**: TF-IDF, similarity-based search, and clustering\n",
        "\n",
        "**Pipeline order** (very important!):\n",
        "```\n",
        "Keyword Search ‚Üí Tokenization ‚Üí Preprocessing ‚Üí N-grams ‚Üí Vectorization (BoW - word counts) ‚Üí Applications (Search, Clustering)\n",
        "```\n",
        "\n",
        "**What's coming NEXT CLASS (Semantic approaches)**:\n",
        "- Embeddings for true semantic search (understanding meaning, synonyms)\n",
        "- \"Space\" and \"cosmic\" will be similar because they share meaning!\n",
        "\n",
        "**Key Point**: In Part 1, we learn **syntactic** models (BOW - word counts). In Part 2, we'll learn TF-IDF. Both work with word patterns but don't understand meaning. **Semantic = meaning**. True semantic search comes in Class 3!\n",
        "\n",
        "**This is top-down learning**: We'll see the problem first, then learn the tools to solve it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8k1yO9-d76p"
      },
      "source": [
        "---\n",
        "\n",
        "## What is Natural Language Processing (NLP)?\n",
        "\n",
        "**NLP** = Teaching computers to understand, interpret, and generate human language\n",
        "\n",
        "### Real-World Applications (Why This Matters!)\n",
        "\n",
        "| Application | Example | Why It's Important |\n",
        "|------------|---------|-------------------|\n",
        "| **Search Engines** | Google, Bing | Finding relevant results for your queries |\n",
        "| **Virtual Assistants** | Siri, Alexa, ChatGPT | Understanding what you're asking |\n",
        "| **Spam Detection** | Email filters | Automatically identifying unwanted emails |\n",
        "| **Sentiment Analysis** | Review analysis | Understanding if reviews are positive/negative |\n",
        "| **Translation** | Google Translate | Converting between languages |\n",
        "| **Text Summarization** | News digests | Condensing long articles into key points |\n",
        "\n",
        "**Today's focus**: Search and clustering - foundational NLP tasks you'll use everywhere!\n",
        "\n",
        "---\n",
        "\n",
        "## Machine Learning in NLP: Two Approaches\n",
        "\n",
        "### Unsupervised Learning (Today's Focus!)\n",
        "- ‚ùå **No labels needed** - we don't tell the model the \"right\" answer\n",
        "- ‚úÖ **Clustering**: Automatically finding groups (e.g., similar movies)\n",
        "- ‚úÖ **Search**: Finding similar documents without examples\n",
        "- üéØ **Goal**: Discover patterns in the data\n",
        "\n",
        "### Supervised Learning (You Know This from Chapter 0!)\n",
        "- ‚úÖ **Needs labeled data** - we provide correct answers\n",
        "- üìä **Text Classification**: Spam/not spam, genre classification\n",
        "- üí≠ **Sentiment Analysis**: Positive/negative/neutral labels\n",
        "- üéØ **Goal**: Learn to predict labels from examples\n",
        "\n",
        "**Key Insight**: Same preprocessing, same vectors - just different goals!\n",
        "- Unsupervised: Find patterns (no labels)\n",
        "- Supervised: Predict labels (with examples)\n",
        "\n",
        "We'll connect these at the end!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxogay2d76q"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AYPWPzded76q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For better output display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyme0gGOd76r"
      },
      "source": [
        "### Load the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h5gze_hnd76s",
        "outputId": "4f406557-f2cc-46cd-889c-ab03b92cca88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file not found. Downloading from GitHub...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('data/movies.csv', <http.client.HTTPMessage at 0x7daf04db58e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Data file downloaded successfully!\n",
            "Loaded 10000 movies\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movie_id                title  \\\n",
              "0         1         Edge of Code   \n",
              "1         2      Storm of Secret   \n",
              "2         3  Under Warrior Redux   \n",
              "3         4      Quest of Secret   \n",
              "4         5          Key of Game   \n",
              "\n",
              "                                         description      genre  rating  \n",
              "0  A compelling romance film about a young advent...    Romance     7.1  \n",
              "1  This captivating romance movie follows a quest...    Romance     6.3  \n",
              "2  In this captivating war story, a secret organi...        War     7.3  \n",
              "3  A compelling fantasy film about a determined d...    Fantasy     8.3  \n",
              "4  A exploration adventure film about a master th...  Adventure     6.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-839dabaa-43e0-45a2-910a-963b1a5ba895\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>genre</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Edge of Code</td>\n",
              "      <td>A compelling romance film about a young advent...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Storm of Secret</td>\n",
              "      <td>This captivating romance movie follows a quest...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Under Warrior Redux</td>\n",
              "      <td>In this captivating war story, a secret organi...</td>\n",
              "      <td>War</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Quest of Secret</td>\n",
              "      <td>A compelling fantasy film about a determined d...</td>\n",
              "      <td>Fantasy</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Key of Game</td>\n",
              "      <td>A exploration adventure film about a master th...</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-839dabaa-43e0-45a2-910a-963b1a5ba895')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-839dabaa-43e0-45a2-910a-963b1a5ba895 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-839dabaa-43e0-45a2-910a-963b1a5ba895');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-71787996-6ce9-4daf-9031-763de3937b4e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-71787996-6ce9-4daf-9031-763de3937b4e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-71787996-6ce9-4daf-9031-763de3937b4e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"movie_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 1,\n        \"max\": 10000,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6253,\n          4685,\n          1732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9983,\n        \"samples\": [\n          \"Agent of Hero\",\n          \"Battle Saga\",\n          \"Secret of Circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9136,\n        \"samples\": [\n          \"A engaging crime film about a secret organization. the film explores themes of courage and redemption.\",\n          \"A captivating western film about a forbidden love. featuring outstanding performances and breathtaking cinematography.\",\n          \"This thrilling action movie follows a small town. full of unexpected twists and turns. features spectacular action sequences and stunts.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"Romance\",\n          \"Sport\",\n          \"Western\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2023180604448267,\n        \"min\": 1.8,\n        \"max\": 10.0,\n        \"num_unique_values\": 79,\n        \"samples\": [\n          4.7,\n          7.1,\n          6.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Load movie descriptions\n",
        "# If running in Google Colab and data file doesn't exist, download it from GitHub\n",
        "import os\n",
        "\n",
        "if not os.path.exists('data/movies.csv'):\n",
        "    print(\"Data file not found. Downloading from GitHub...\")\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    import urllib.request\n",
        "    url = 'https://raw.githubusercontent.com/samsung-ai-course/8th-9th-edition/main/Chapter%202%20-%20Natural%20Language%20Processing/Class%201%20%26%202%20-%20NLP%20and%20Search/data/movies.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/movies.csv')\n",
        "    print(\"‚úì Data file downloaded successfully!\")\n",
        "\n",
        "df = pd.read_csv('data/movies.csv')\n",
        "print(f\"Loaded {len(df)} movies\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DE07heWd76s"
      },
      "source": [
        "## Why Text is Hard\n",
        "\n",
        "Computers work with numbers, but text is made of words. This creates several challenges:\n",
        "\n",
        "1. **Unstructured**: Text doesn't have a fixed format\n",
        "2. **Synonyms**: \"space\" vs \"cosmic\" vs \"galactic\" - different words, similar meaning\n",
        "3. **Context**: \"bank\" could mean financial institution or river edge\n",
        "4. **Variations**: \"sci-fi\", \"science fiction\", \"Science Fiction\" - same concept\n",
        "5. **Word order matters**: \"dog bites man\" vs \"man bites dog\" - completely different!\n",
        "\n",
        "Let's look at our movie descriptions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xGDfg5Grd76t",
        "outputId": "5f445fd3-0f43-4e5d-9fe0-bf6c7ee49ad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie Description Example:\n",
            "============================================================\n",
            "Title: Edge of Code\n",
            "Description: A compelling romance film about a young adventurer. an epic adventure that spans continents and generations. a touching love story that will warm your heart.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Let's examine a movie description\n",
        "print(\"Movie Description Example:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Title: {df.loc[0, 'title']}\")\n",
        "print(f\"Description: {df.loc[0, 'description']}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHopmg-d76u"
      },
      "source": [
        "## Two Approaches to Search\n",
        "\n",
        "### 1. Keyword Search (Simple but Limited)\n",
        "- Looks for exact word matches\n",
        "- Can use simple substring matching or Term Frequency (TF) approaches\n",
        "- Fast and simple\n",
        "- Fails with synonyms (\"space movie\" won't find \"cosmic adventure\")\n",
        "- **Syntactic only** - works with words, not meaning\n",
        "\n",
        "### 2. True Semantic Search (Class 3 - Embeddings!)\n",
        "- **Semantic = meaning-based** - understands synonyms and related concepts\n",
        "- \"Space\" and \"cosmic\" are close in meaning (semantic similarity)\n",
        "- Requires embeddings (dense vectors that capture meaning)\n",
        "- This is what we'll learn in Class 3!\n",
        "\n",
        "**Key distinction:**\n",
        "- **Syntactic models** (BOW): Work with word presence/counts, no understanding of meaning\n",
        "- **Semantic models** (embeddings): Understand meaning - synonyms and related concepts are similar\n",
        "\n",
        "Let's start with different keyword search approaches:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEu-bU3zd76v"
      },
      "source": [
        "### Approach 1: Simple Substring Matching (Vanilla Keyword Search)\n",
        "\n",
        "The most basic approach - just check if the query word appears in the text.\n",
        "\n",
        "**Concept**: Simple substring matching checks if query words appear anywhere in the document text. It's fast but limited - it only finds exact word matches and doesn't rank results.\n",
        "\n",
        "**Implementation**: You'll implement this in **Exercise 4** in the Exercise Notebook!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i7khZKjld76v",
        "outputId": "f7732d40-c6b3-416d-a249-707d85eccd63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 564 results for 'space':\n",
            "   movie_id            title  match\n",
            "0        25     Legacy Falls   True\n",
            "1        41   Fear of Knight   True\n",
            "2        47      Light: Soul   True\n",
            "3        65  Mage of Warrior   True\n",
            "4        78   Warrior of War   True\n"
          ]
        }
      ],
      "source": [
        "# Simple keyword search - Let's implement this together!\n",
        "def simple_keyword_search(df, query, column='description'):\n",
        "    \"\"\"\n",
        "    Simple keyword search: finds documents containing the query words (exact match)\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = str(row[column]).lower()\n",
        "\n",
        "        if query_lower in text:\n",
        "            results.append({\n",
        "                'movie_id': row['movie_id'],\n",
        "                'title': row['title'],\n",
        "                'match': True\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Let's test it together!\n",
        "query = \"space\"\n",
        "results = simple_keyword_search(df, query)\n",
        "print(f\"Found {len(results)} results for '{query}':\")\n",
        "print(results.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByHfL7p-d76v"
      },
      "source": [
        "### Approach 2: Multiple Keyword Search\n",
        "\n",
        "Instead of searching for a single word, we can search for multiple words at once. The query \"space adventure\" should find documents containing both \"space\" AND \"adventure\".\n",
        "\n",
        "**Concept**: Multiple keyword search finds documents that contain ALL query words. This is more precise than single-word search and allows for more specific queries.\n",
        "\n",
        "**Implementation**: You'll implement this in **Exercise 4** in the Exercise Notebook!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BbSUV0zod76w",
        "outputId": "ff14f12c-a531-4284-d09c-dab07cd406fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 173 results for 'space adventure' (containing ALL words):\n",
            "   movie_id            title  match\n",
            "0        41   Fear of Knight   True\n",
            "1        65  Mage of Warrior   True\n",
            "2       103   Fire of Battle   True\n",
            "3       148    Square of War   True\n",
            "4       284        War: Soul   True\n"
          ]
        }
      ],
      "source": [
        "# Multiple keyword search - Complete solution\n",
        "def multiple_keyword_search(df, query, column='description'):\n",
        "    \"\"\"\n",
        "    Multiple keyword search: finds documents containing ALL query words (AND logic)\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "    query_words = query_lower.split()  # Split query into individual words\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = str(row[column]).lower()\n",
        "\n",
        "        # Check if ALL query words appear in the text\n",
        "        all_words_found = all(word in text for word in query_words)\n",
        "\n",
        "        if all_words_found:\n",
        "            results.append({\n",
        "                'movie_id': row['movie_id'],\n",
        "                'title': row['title'],\n",
        "                'match': True\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Let's test it!\n",
        "query_multi = \"space adventure\"\n",
        "results_multi = multiple_keyword_search(df, query_multi)\n",
        "print(f\"Found {len(results_multi)} results for '{query_multi}' (containing ALL words):\")\n",
        "print(results_multi.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS64AQgNd76w"
      },
      "source": [
        "## Simple Tokenization\n",
        "\n",
        "Before we move to more advanced preprocessing, let's understand the basics of tokenization - splitting text into individual words.\n",
        "\n",
        "**Tokenization** is the process of breaking text into smaller units (tokens), which are usually words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3mYJPCXId76w",
        "outputId": "e4339770-2a44-4a7b-ac95-9d0025de6dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Natural Language Processing is amazing! It's used everywhere.\n",
            "\n",
            "Simple split: ['Natural', 'Language', 'Processing', 'is', 'amazing!', \"It's\", 'used', 'everywhere.']\n",
            "Regex tokenization: ['natural', 'language', 'processing', 'is', 'amazing', 'it', 's', 'used', 'everywhere']\n",
            "\n",
            "üí° Key Insight: Tokenization is the first step in converting text to numbers!\n",
            "   We'll use this when we create Bag of Words vectors.\n"
          ]
        }
      ],
      "source": [
        "# Simple tokenization examples\n",
        "text = \"Natural Language Processing is amazing! It's used everywhere.\"\n",
        "print(\"Original:\", text)\n",
        "\n",
        "# Split on whitespace (simple but loses punctuation)\n",
        "tokens_simple = text.split()\n",
        "print(\"\\nSimple split:\", tokens_simple)\n",
        "\n",
        "# Regex tokenization (find all word characters)\n",
        "tokens_regex = re.findall(r'\\w+', text.lower())\n",
        "print(\"Regex tokenization:\", tokens_regex)\n",
        "\n",
        "print(\"\\nüí° Key Insight: Tokenization is the first step in converting text to numbers!\")\n",
        "print(\"   We'll use this when we create Bag of Words vectors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQCM3UmVd76w"
      },
      "source": [
        "**Key Insights about Keyword Search**:\n",
        "\n",
        "**Advantages**:\n",
        "- ‚úÖ Fast and simple\n",
        "- ‚úÖ Finds exact word matches\n",
        "- ‚úÖ Multiple keyword search allows more specific queries\n",
        "\n",
        "**Limitations**:\n",
        "- ‚ùå \"space\" won't match \"cosmic\" or \"galactic\" (synonyms) - **syntactic only, no meaning**\n",
        "- ‚ùå \"adventure\" won't match \"journey\" or \"quest\" - different words = zero similarity\n",
        "- ‚ùå Doesn't understand meaning - it's a **syntactic model** (word-based, not meaning-based)\n",
        "- ‚ùå No ranking - all matches are equal\n",
        "\n",
        "To do better, we need to:\n",
        "1. **Preprocess** the text properly (tokenization, normalization)\n",
        "2. **Convert** text to numerical vectors (Bag of Words - word counts)\n",
        "3. **Measure similarity** between vectors (we'll learn this in Part 2!)\n",
        "\n",
        "This moves us to **vectorization** - converting text to numbers. In Part 2, we'll learn TF-IDF for better search and clustering!\n",
        "\n",
        "---\n",
        "\n",
        "## Text Preprocessing Pipeline\n",
        "\n",
        "Before we can work with text, we need to clean and prepare it. The pipeline has three main stages:\n",
        "\n",
        "1. **Pre-processing** (before tokenization): Clean the raw text\n",
        "2. **Tokenization**: Split text into individual words/tokens\n",
        "3. **Post-processing** (after tokenization): Further refine the tokens\n",
        "\n",
        "### Why Preprocessing Matters\n",
        "\n",
        "**Garbage In = Garbage Out**: If we don't clean our text properly, our models will learn from noise, not signal!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FtVERSud76x"
      },
      "source": [
        "### Stage 1: Pre-processing (Before Tokenization)\n",
        "\n",
        "**Goal**: Clean the raw text before splitting it into words\n",
        "\n",
        "Common tasks:\n",
        "- Remove HTML tags\n",
        "- Remove special characters\n",
        "- Normalize URLs, emails, phone numbers (using **Regular Expressions/Regex**)\n",
        "- Handle case (convert to lowercase)\n",
        "- Remove extra whitespace\n",
        "\n",
        "Let's see an example with **Regular Expressions (Regex)**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7VzaqT-Td76x",
        "outputId": "786a96d1-e6a9-4b1f-f717-ec31cc70adfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Contact us at info@example.com or call (555) 123-4567. Visit https://example.com for more info!!!\n",
            "\n",
            "After removing URLs: Contact us at info@example.com or call (555) 123-4567. Visit  for more info!!!\n",
            "After removing emails: Contact us at  or call (555) 123-4567. Visit  for more info!!!\n",
            "After removing phones: Contact us at  or call . Visit  for more info!!!\n",
            "After removing punctuation: Contact us at  or call   Visit  for more info   \n",
            "Final (lowercase, normalized): contact us at or call visit for more info\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing with regex - Complete solution\n",
        "sample_text = \"Contact us at info@example.com or call (555) 123-4567. Visit https://example.com for more info!!!\"\n",
        "print(\"Original:\", sample_text)\n",
        "\n",
        "# Remove URLs using regex\n",
        "# Pattern: r'https?://\\S+' matches http:// or https:// followed by non-whitespace\n",
        "text_no_urls = re.sub(r'https?://\\S+', '', sample_text)\n",
        "print(\"\\nAfter removing URLs:\", text_no_urls)\n",
        "\n",
        "# Remove email addresses\n",
        "# Pattern: r'\\S+@\\S+' matches non-whitespace characters before and after @\n",
        "text_no_emails = re.sub(r'\\S+@\\S+', '', text_no_urls)\n",
        "print(\"After removing emails:\", text_no_emails)\n",
        "\n",
        "# Remove phone numbers\n",
        "# Pattern handles: (555) 123-4567 or 555-123-4567\n",
        "text_no_phones = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '', text_no_emails)\n",
        "print(\"After removing phones:\", text_no_phones)\n",
        "\n",
        "# Remove punctuation but keep letters, numbers, spaces\n",
        "# Pattern: [^\\w\\s] means \"not word characters or whitespace\"\n",
        "text_clean = re.sub(r'[^\\w\\s]', ' ', text_no_phones)\n",
        "print(\"After removing punctuation:\", text_clean)\n",
        "\n",
        "# Normalize whitespace and lowercase\n",
        "# Pattern: r'\\s+' matches one or more whitespace characters\n",
        "text_final = re.sub(r'\\s+', ' ', text_clean).strip().lower()\n",
        "print(\"Final (lowercase, normalized):\", text_final)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zixgwg1md76x"
      },
      "source": [
        "### Stage 2: Tokenization\n",
        "\n",
        "**Goal**: Split text into individual words (tokens)\n",
        "\n",
        "Tokens can be:\n",
        "- Words: \"machine\", \"learning\"\n",
        "- Punctuation: \".\", \",\"\n",
        "- Numbers: \"2024\"\n",
        "- Subwords: \"un-\" + \"happiness\" (advanced)\n",
        "\n",
        "Simple approach: split on whitespace\n",
        "Better approach: use regex to find word boundaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c05b7WzPd76x",
        "outputId": "e9f73024-e1bf-4e42-d27f-e3b77f7dff05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Natural Language Processing is amazing! It's used everywhere.\n",
            "\n",
            "Simple split: ['Natural', 'Language', 'Processing', 'is', 'amazing!', \"It's\", 'used', 'everywhere.']\n",
            "Regex tokenization: ['natural', 'language', 'processing', 'is', 'amazing', 'it', 's', 'used', 'everywhere']\n"
          ]
        }
      ],
      "source": [
        "# Simple tokenization examples\n",
        "text = \"Natural Language Processing is amazing! It's used everywhere.\"\n",
        "print(\"Original:\", text)\n",
        "\n",
        "# Split on whitespace (simple but loses punctuation)\n",
        "tokens_simple = text.split()\n",
        "print(\"\\nSimple split:\", tokens_simple)\n",
        "\n",
        "# Regex tokenization (find all word characters)\n",
        "tokens_regex = re.findall(r'\\w+', text.lower())\n",
        "print(\"Regex tokenization:\", tokens_regex)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlihyVj1d76y"
      },
      "source": [
        "### Stage 3: Post-processing (After Tokenization)\n",
        "\n",
        "**Goal**: Further refine tokens by removing noise\n",
        "\n",
        "Common tasks:\n",
        "- Remove **stop words**: \"the\", \"a\", \"an\", \"and\", \"or\" (common but not informative)\n",
        "- Remove very short tokens: \"I\", \"a\" (often noise)\n",
        "- Stemming/Lemmatization: \"running\" ‚Üí \"run\" (we'll skip this for now)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Oi42tVRbd76y",
        "outputId": "9ef0f2ff-c85e-4d24-e517-cf34ad4f45c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: A compelling romance film about a young adventurer. an epic adventure that spans continents and generations. a touching love story that will warm your heart.\n",
            "\n",
            "Preprocessed tokens: ['compelling', 'romance', 'film', 'about', 'young', 'adventurer', 'epic', 'adventure', 'spans', 'continents', 'generations', 'touching', 'love', 'story', 'warm', 'your', 'heart']\n"
          ]
        }
      ],
      "source": [
        "# Complete preprocessing pipeline - Complete solution\n",
        "STOP_WORDS = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "              'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "              'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
        "              'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',\n",
        "              'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who'}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline: clean, tokenize, filter\n",
        "    \"\"\"\n",
        "    # Step 1: Pre-processing - lowercase and normalize\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Step 2 - Tokenization: Use regex to find all words\n",
        "    # Pattern: \\w+ matches word characters\n",
        "    tokens = re.findall(r'\\w+', text)\n",
        "\n",
        "    # Step 3 - Post-processing: Remove stop words and very short tokens (length < 3)\n",
        "    tokens_clean = [token for token in tokens\n",
        "                    if token not in STOP_WORDS and len(token) > 2]\n",
        "\n",
        "    return tokens_clean\n",
        "\n",
        "# Let's test it!\n",
        "sample = df.loc[0, 'description']\n",
        "print(\"Original:\", sample)\n",
        "print(\"\\nPreprocessed tokens:\", preprocess_text(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqnWgGVmd76z"
      },
      "source": [
        "---\n",
        "\n",
        "## N-grams: Capturing Word Order (Feature Extraction)\n",
        "\n",
        "**After preprocessing and tokenization**, we can create **n-grams** - sequences of n consecutive tokens. N-grams help capture some word order information that simple Bag of Words loses!\n",
        "\n",
        "### What are N-grams?\n",
        "\n",
        "**N-grams** = Sequences of N consecutive tokens (words or characters)\n",
        "\n",
        "**Types:**\n",
        "- **Unigrams (1-grams)**: Single words - \"I\", \"love\", \"Python\"\n",
        "- **Bigrams (2-grams)**: Pairs of consecutive words - \"I love\", \"love Python\"\n",
        "- **Trigrams (3-grams)**: Triples of consecutive words - \"I love Python\"\n",
        "\n",
        "### Why N-grams Matter\n",
        "\n",
        "**Problem with unigrams (single words)**: \"Dog bites man\" and \"Man bites dog\" have the same unigrams!\n",
        "\n",
        "**Solution with bigrams**:\n",
        "- \"Dog bites man\" ‚Üí [\"dog bites\", \"bites man\"]\n",
        "- \"Man bites dog\" ‚Üí [\"man bites\", \"bites dog\"]\n",
        "- Different bigrams ‚Üí different meaning captured!\n",
        "\n",
        "### Example:\n",
        "\n",
        "```\n",
        "Text: \"I love Python programming\"\n",
        "\n",
        "Unigrams (1-grams):\n",
        "  [\"i\", \"love\", \"python\", \"programming\"]\n",
        "\n",
        "Bigrams (2-grams):\n",
        "  [\"i love\", \"love python\", \"python programming\"]\n",
        "\n",
        "Trigrams (3-grams):\n",
        "  [\"i love python\", \"love python programming\"]\n",
        "```\n",
        "\n",
        "**Trade-off**:\n",
        "- ‚úÖ Better capture of word order and context\n",
        "- ‚ùå Larger vocabulary (more features)\n",
        "- ‚ùå Can be sparse (many n-grams appear only once)\n",
        "\n",
        "**Common Usage**: Combining unigrams + bigrams gives a good balance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uNKojFwHd76z",
        "outputId": "3eab3319-6d5f-4a02-93e6-cc8e5cc6713c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: I love Python programming\n",
            "After preprocessing: ['love', 'python', 'programming']\n",
            "\n",
            "============================================================\n",
            "Unigrams (1-grams) - single words:\n",
            "  ['love', 'python', 'programming']\n",
            "\n",
            "Bigrams (2-grams) - pairs of consecutive words:\n",
            "  ['love python', 'python programming']\n",
            "\n",
            "Trigrams (3-grams) - triplets of consecutive words:\n",
            "  ['love python programming']\n",
            "\n",
            "============================================================\n",
            "Why N-grams Matter - Word Order Example:\n",
            "============================================================\n",
            "\n",
            "Text 1: 'The dog bites the man'\n",
            "Tokens: ['dog', 'bites', 'man']\n",
            "Bigrams: ['dog bites', 'bites man']\n",
            "\n",
            "Text 2: 'The man bites the dog'\n",
            "Tokens: ['man', 'bites', 'dog']\n",
            "Bigrams: ['man bites', 'bites dog']\n",
            "\n",
            "‚Üí Different bigrams = different meaning captured!\n",
            "‚Üí N-grams help preserve some word order information\n",
            "\n",
            "============================================================\n",
            "N-grams with Real Data:\n",
            "============================================================\n",
            "Movie description: A compelling romance film about a young adventurer. an epic adventure that spans continents and gene...\n",
            "\n",
            "Tokens (first 10): ['compelling', 'romance', 'film', 'about', 'young', 'adventurer', 'epic', 'adventure', 'spans', 'continents']\n",
            "\n",
            "Unigrams (first 10): ['compelling', 'romance', 'film', 'about', 'young', 'adventurer', 'epic', 'adventure', 'spans', 'continents']\n",
            "Bigrams (first 10): ['compelling romance', 'romance film', 'film about', 'about young', 'young adventurer', 'adventurer epic', 'epic adventure', 'adventure spans', 'spans continents', 'continents generations']\n",
            "\n",
            "Total unigrams: 17\n",
            "Total bigrams: 16\n",
            "\n",
            "üí° Note: N-grams are created AFTER tokenization and BEFORE vectorization.\n",
            "   They're used as features in TF-IDF (you'll see this in scikit-learn options).\n"
          ]
        }
      ],
      "source": [
        "# N-grams Implementation - Complete solution\n",
        "\n",
        "def create_ngrams(tokens, n=2):\n",
        "    \"\"\"\n",
        "    Create n-grams from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens: List of tokens (words)\n",
        "        n: Size of n-gram (1=unigrams, 2=bigrams, 3=trigrams)\n",
        "\n",
        "    Returns:\n",
        "        list: List of n-grams\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        ngrams = []\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            # Create n-gram by joining n consecutive tokens\n",
        "            ngram = ' '.join(tokens[i:i+n])\n",
        "            ngrams.append(ngram)\n",
        "        return ngrams\n",
        "\n",
        "# Example 1: Creating n-grams from text\n",
        "text = \"I love Python programming\"\n",
        "tokens = preprocess_text(text)  # Preprocess first (tokenize, remove stop words, etc.)\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"After preprocessing: {tokens}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Create different n-grams\n",
        "unigrams = create_ngrams(tokens, n=1)\n",
        "bigrams = create_ngrams(tokens, n=2)\n",
        "trigrams = create_ngrams(tokens, n=3)\n",
        "\n",
        "print(\"Unigrams (1-grams) - single words:\")\n",
        "print(f\"  {unigrams}\")\n",
        "\n",
        "print(\"\\nBigrams (2-grams) - pairs of consecutive words:\")\n",
        "print(f\"  {bigrams}\")\n",
        "\n",
        "print(\"\\nTrigrams (3-grams) - triplets of consecutive words:\")\n",
        "print(f\"  {trigrams}\")\n",
        "\n",
        "# Example 2: Demonstrating why n-grams matter (word order)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Why N-grams Matter - Word Order Example:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "text1 = \"The dog bites the man\"\n",
        "text2 = \"The man bites the dog\"\n",
        "\n",
        "tokens1 = preprocess_text(text1)\n",
        "tokens2 = preprocess_text(text2)\n",
        "\n",
        "bigrams1 = create_ngrams(tokens1, n=2)\n",
        "bigrams2 = create_ngrams(tokens2, n=2)\n",
        "\n",
        "print(f\"\\nText 1: '{text1}'\")\n",
        "print(f\"Tokens: {tokens1}\")\n",
        "print(f\"Bigrams: {bigrams1}\")\n",
        "\n",
        "print(f\"\\nText 2: '{text2}'\")\n",
        "print(f\"Tokens: {tokens2}\")\n",
        "print(f\"Bigrams: {bigrams2}\")\n",
        "\n",
        "print(\"\\n‚Üí Different bigrams = different meaning captured!\")\n",
        "print(\"‚Üí N-grams help preserve some word order information\")\n",
        "\n",
        "# Example 3: Using n-grams with movie descriptions\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"N-grams with Real Data:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_movie = df.loc[0, 'description']\n",
        "tokens_movie = preprocess_text(sample_movie)\n",
        "print(f\"Movie description: {sample_movie[:100]}...\")\n",
        "print(f\"\\nTokens (first 10): {tokens_movie[:10]}\")\n",
        "\n",
        "movie_unigrams = create_ngrams(tokens_movie, n=1)\n",
        "movie_bigrams = create_ngrams(tokens_movie, n=2)\n",
        "\n",
        "print(f\"\\nUnigrams (first 10): {movie_unigrams[:10]}\")\n",
        "print(f\"Bigrams (first 10): {movie_bigrams[:10]}\")\n",
        "print(f\"\\nTotal unigrams: {len(movie_unigrams)}\")\n",
        "print(f\"Total bigrams: {len(movie_bigrams)}\")\n",
        "\n",
        "print(\"\\nüí° Note: N-grams are created AFTER tokenization and BEFORE vectorization.\")\n",
        "print(\"   They're used as features in TF-IDF (you'll see this in scikit-learn options).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXnbYpfld760"
      },
      "source": [
        "### N-grams in Production\n",
        "\n",
        "In practice, you'll use libraries like scikit-learn's `CountVectorizer` or `TfidfVectorizer` which can automatically create n-grams:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sMgl2GPPd760",
        "outputId": "ff5996f1-8058-446a-afc3-620e5c62b910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer with ngram_range=(1, 2) created!\n",
            "This will create both unigrams and bigrams when you fit it to text.\n"
          ]
        }
      ],
      "source": [
        "# Example (we'll see this in Part 2):\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create unigrams and bigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Unigrams + bigrams\n",
        "print(\"TfidfVectorizer with ngram_range=(1, 2) created!\")\n",
        "print(\"This will create both unigrams and bigrams when you fit it to text.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9ydf4gwd760"
      },
      "source": [
        "**Common choices:**\n",
        "- `ngram_range=(1, 1)`: Only unigrams (standard BoW)\n",
        "- `ngram_range=(1, 2)`: Unigrams + bigrams (good balance)\n",
        "- `ngram_range=(2, 2)`: Only bigrams\n",
        "- `ngram_range=(1, 3)`: Unigrams + bigrams + trigrams (more features, can be sparse)\n",
        "\n",
        "**Key Insight**: N-grams are created **after tokenization** and **before vectorization** (BoW). They help capture word order, but still don't capture semantic meaning - that requires embeddings (Class 3)!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i7vra-gd760"
      },
      "source": [
        "---\n",
        "\n",
        "## From Text to Numbers: Bag of Words (BoW) - Vectorization\n",
        "\n",
        "**Now that we have preprocessed tokens (and optionally n-grams)**, we need to convert them to numbers that computers can work with. This is called **vectorization**.\n",
        "\n",
        "**Pipeline reminder:**\n",
        "1. ‚úÖ Keyword search (simple and multiple)\n",
        "2. ‚úÖ Simple tokenization\n",
        "3. ‚úÖ Preprocessing (clean, normalize)\n",
        "4. ‚úÖ Post-processing (filter, remove stop words)\n",
        "5. ‚úÖ Feature extraction (n-grams - optional, we just covered this!)\n",
        "6. **‚Üê We are here**: Vectorization (Bag of Words - word counts)\n",
        "7. Applications (Similarity Search, Clustering - Part 2)\n",
        "\n",
        "### The Bag of Words Model (BoW)\n",
        "\n",
        "**Idea**: Represent each document as a vector of word counts, ignoring word order.\n",
        "\n",
        "The process:\n",
        "1. Create vocabulary (list of all unique words from the corpus)\n",
        "2. Count how many times each word appears in a document\n",
        "3. Represent document as vector of counts (not frequencies, just counts!)\n",
        "\n",
        "**Terminology reminder:**\n",
        "- **Corpus**: Collection of all documents we're working with\n",
        "- **Vocabulary**: All unique words/tokens in the corpus\n",
        "- **Token**: Individual word after tokenization\n",
        "\n",
        "Example:\n",
        "- Document 1: \"I love Python\"\n",
        "- Document 2: \"Python is great\"\n",
        "- **Corpus**: The collection of both documents\n",
        "- **Vocabulary**: [\"i\", \"love\", \"python\", \"is\", \"great\"] (all unique words from the corpus)\n",
        "\n",
        "**Bag of Words vectors**:\n",
        "- Doc 1: [1, 1, 1, 0, 0]  (one \"i\", one \"love\", one \"python\")\n",
        "- Doc 2: [0, 0, 1, 1, 1]  (one \"python\", one \"is\", one \"great\")\n",
        "\n",
        "**Key insight**: We've lost word order! \"Dog bites man\" = \"Man bites dog\" in BoW. But for many tasks, this is okay!\n",
        "\n",
        "**Remember**: BoW is just counting how many times each word appears - simple word counts, not frequencies!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KwiGu94pd760",
        "outputId": "edc67117-5722-4d82-985f-0db37e4c0b1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 3 documents\n",
            "Vocabulary: ['a', 'i', 'is', 'language', 'learning', 'love', 'machine', 'programming', 'python']\n",
            "Vocabulary size: 9\n",
            "\n",
            "'I love Python programming' -> [0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
            "\n",
            "'Python is a programming language' -> [1, 0, 1, 1, 0, 0, 0, 1, 1]\n",
            "\n",
            "'I love machine learning' -> [0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
            "\n",
            "Bag of Words Matrix:\n",
            "   a  i  is  language  learning  love  machine  programming  python\n",
            "0  0  1   0         0         0     1        0            1       1\n",
            "1  1  0   1         1         0     0        0            1       1\n",
            "2  0  1   0         0         1     1        1            0       0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    # lowercase + ficar s√≥ com letras/n√∫meros + tokenizar por espa√ßos\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # remove pontua√ß√£o\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Bag of Words\n",
        "docs = [\n",
        "    \"I love Python programming\",\n",
        "    \"Python is a programming language\",\n",
        "    \"I love machine learning\"\n",
        "]\n",
        "\n",
        "# Step 1: Build vocabulary from the corpus (all unique words)\n",
        "all_words = set()\n",
        "for doc in docs:\n",
        "    tokens = preprocess_text(doc)\n",
        "    all_words.update(tokens)\n",
        "\n",
        "vocab = sorted(list(all_words))\n",
        "print(f\"Corpus: {len(docs)} documents\")\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Step 2: Create BoW vectors for each document\n",
        "bow_vectors = []\n",
        "for doc in docs:\n",
        "    tokens = preprocess_text(doc)     # 1) preprocess\n",
        "    word_counts = Counter(tokens)     # 2) count\n",
        "\n",
        "    # 3) build vector aligned with vocab\n",
        "    vector = [word_counts.get(word, 0) for word in vocab]\n",
        "\n",
        "    bow_vectors.append(vector)\n",
        "    print(f\"\\n'{doc}' -> {vector}\")\n",
        "\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(pd.DataFrame(bow_vectors, columns=vocab))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Kth_0Td760"
      },
      "source": [
        "## Sparse vs Dense Vectors\n",
        "\n",
        "This is a crucial concept in NLP!\n",
        "\n",
        "### Sparse Vectors (like BoW)\n",
        "- **Most values are zero** (e.g., [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...])\n",
        "- Vocabulary size can be huge (10,000-100,000+ words)\n",
        "- Each document only uses a small fraction of the vocabulary\n",
        "- **Memory efficient** when stored in sparse format (only store non-zero values)\n",
        "- Example: Bag of Words, TF-IDF\n",
        "\n",
        "### Dense Vectors (like Embeddings - we'll see this next class!)\n",
        "- **Most/all values are non-zero** (e.g., [0.23, -0.15, 0.87, ..., 0.42])\n",
        "- Fixed, smaller dimension (typically 100-768 dimensions)\n",
        "- Each dimension has meaning (learned representation)\n",
        "- **Captures relationships** between words\n",
        "- Example: Word embeddings, sentence embeddings\n",
        "\n",
        "Let's visualize this:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AijATGead761"
      },
      "source": [
        "## Comparing: With vs Without Preprocessing\n",
        "\n",
        "**Key Question**: Does preprocessing make a difference? Let's find out!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vaCbkURVd761",
        "outputId": "bea270eb-ad1f-4227-9efe-87f34c4b0d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "WITHOUT Preprocessing (raw text):\n",
            "======================================================================\n",
            "Corpus: 3 documents\n",
            "\n",
            "Vocabulary: ['a', 'amazing!!!', 'i', 'is', \"it's\", 'language.', 'learning!', 'love', 'machine', 'programming', 'python', 'python!']\n",
            "Vocabulary size: 12\n",
            "\n",
            "Notice: 'i', 'python!', 'it's', 'amazing!!!', 'language.' are separate words!\n",
            "Punctuation and case variations create different words!\n",
            "\n",
            "BoW Matrix (no preprocessing):\n",
            "       a  amazing!!!  i  is  it's  language.  learning!  love  machine  \\\n",
            "Doc 1  0           1  1   0     1          0          0     1        0   \n",
            "Doc 2  1           0  0   1     0          1          0     0        0   \n",
            "Doc 3  0           0  1   0     0          0          1     1        1   \n",
            "\n",
            "       programming  python  python!  \n",
            "Doc 1            0       0        1  \n",
            "Doc 2            1       1        0  \n",
            "Doc 3            0       0        0  \n",
            "\n",
            "======================================================================\n",
            "WITH Preprocessing (cleaned and tokenized):\n",
            "======================================================================\n",
            "Corpus: 3 documents\n",
            "\n",
            "Vocabulary: ['a', 'amazing', 'i', 'is', 'its', 'language', 'learning', 'love', 'machine', 'programming', 'python']\n",
            "Vocabulary size: 11\n",
            "\n",
            "Notice: Clean words only! Punctuation removed, stop words filtered!\n",
            "\n",
            "BoW Matrix (with preprocessing):\n",
            "       a  amazing  i  is  its  language  learning  love  machine  programming  \\\n",
            "Doc 1  0        1  1   0    1         0         0     1        0            0   \n",
            "Doc 2  1        0  0   1    0         1         0     0        0            1   \n",
            "Doc 3  0        0  1   0    0         0         1     1        1            0   \n",
            "\n",
            "       python  \n",
            "Doc 1       1  \n",
            "Doc 2       1  \n",
            "Doc 3       0  \n",
            "\n",
            "======================================================================\n",
            "KEY INSIGHT:\n",
            "======================================================================\n",
            "Without preprocessing: More vocabulary words, many variations of same word\n",
            "With preprocessing: Cleaner vocabulary, focuses on meaningful words\n",
            "‚Üí Preprocessing reduces noise and makes patterns clearer!\n"
          ]
        }
      ],
      "source": [
        "# Comparison: BoW with preprocessing vs without preprocessing\n",
        "# Our corpus: collection of sample documents\n",
        "docs_sample = [\n",
        "    \"I LOVE Python! It's amazing!!!\",\n",
        "    \"Python is a programming language.\",\n",
        "    \"i love machine learning!\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"WITHOUT Preprocessing (raw text):\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Corpus: {len(docs_sample)} documents\\n\")\n",
        "\n",
        "# BoW without preprocessing - just split on whitespace\n",
        "# Build vocabulary from corpus\n",
        "all_words_no_preprocess = set()\n",
        "for doc in docs_sample:\n",
        "    words = doc.lower().split()  # Simple split, no cleaning\n",
        "    all_words_no_preprocess.update(words)\n",
        "\n",
        "vocab_no_preprocess = sorted(list(all_words_no_preprocess))\n",
        "print(f\"Vocabulary: {vocab_no_preprocess}\")\n",
        "print(f\"Vocabulary size: {len(vocab_no_preprocess)}\")\n",
        "print(f\"\\nNotice: 'i', 'python!', 'it's', 'amazing!!!', 'language.' are separate words!\")\n",
        "print(f\"Punctuation and case variations create different words!\")\n",
        "\n",
        "bow_no_preprocess = []\n",
        "for doc in docs_sample:\n",
        "    words = doc.lower().split()\n",
        "    word_counts = Counter(words)\n",
        "    vector = [word_counts.get(word, 0) for word in vocab_no_preprocess]\n",
        "    bow_no_preprocess.append(vector)\n",
        "\n",
        "print(\"\\nBoW Matrix (no preprocessing):\")\n",
        "print(pd.DataFrame(bow_no_preprocess, columns=vocab_no_preprocess,\n",
        "                   index=[f\"Doc {i+1}\" for i in range(len(docs_sample))]))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"WITH Preprocessing (cleaned and tokenized):\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Corpus: {len(docs_sample)} documents\\n\")\n",
        "\n",
        "# BoW with preprocessing\n",
        "# Build vocabulary from corpus (after preprocessing)\n",
        "all_words_preprocess = set()\n",
        "for doc in docs_sample:\n",
        "    tokens = preprocess_text(doc)\n",
        "    all_words_preprocess.update(tokens)\n",
        "\n",
        "vocab_preprocess = sorted(list(all_words_preprocess))\n",
        "print(f\"Vocabulary: {vocab_preprocess}\")\n",
        "print(f\"Vocabulary size: {len(vocab_preprocess)}\")\n",
        "print(f\"\\nNotice: Clean words only! Punctuation removed, stop words filtered!\")\n",
        "\n",
        "bow_preprocess = []\n",
        "for doc in docs_sample:\n",
        "    tokens = preprocess_text(doc)\n",
        "    word_counts = Counter(tokens)\n",
        "    vector = [word_counts.get(word, 0) for word in vocab_preprocess]\n",
        "    bow_preprocess.append(vector)\n",
        "\n",
        "print(\"\\nBoW Matrix (with preprocessing):\")\n",
        "print(pd.DataFrame(bow_preprocess, columns=vocab_preprocess,\n",
        "                   index=[f\"Doc {i+1}\" for i in range(len(docs_sample))]))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"KEY INSIGHT:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Without preprocessing: More vocabulary words, many variations of same word\")\n",
        "print(\"With preprocessing: Cleaner vocabulary, focuses on meaningful words\")\n",
        "print(\"‚Üí Preprocessing reduces noise and makes patterns clearer!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfaDTAINd761"
      },
      "source": [
        "## The Problem with Syntactic Representations (BoW Limitations)\n",
        "\n",
        "**Critical Understanding**: Bag of Words is a **syntactic representation** (TF-IDF in Part 2 is also syntactic) - they only capture word presence/counts, NOT meaning!\n",
        "\n",
        "**Key Terminology:**\n",
        "- **Syntactic**: Based on word structure/counts (BOW) or frequencies (TF-IDF) - no understanding of meaning\n",
        "- **Semantic**: Based on meaning - understands synonyms and related concepts (embeddings - Class 3)\n",
        "\n",
        "**Remember**: Semantic = meaning. Syntactic models like BOW work with word counts only - they don't understand meaning!\n",
        "\n",
        "Let's see the issues:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5SLIUlsMd761",
        "outputId": "b3fb1d72-2ab2-4b0e-b721-2f9b4dc83dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Problem 1: Word Order is Lost\n",
            "======================================================================\n",
            "Vocabulary: ['bites', 'dog', 'man', 'the']\n",
            "\n",
            "'The dog bites the man'\n",
            "‚Üí [1, 1, 1, 2]\n",
            "\n",
            "'The man bites the dog'\n",
            "‚Üí [1, 1, 1, 2]\n",
            "\n",
            "==================================================\n",
            "Result: Both sentences have IDENTICAL BoW vectors!\n",
            "But they have COMPLETELY DIFFERENT meanings!\n",
            "==================================================\n",
            "\n",
            "======================================================================\n",
            "Problem 2: Synonyms are Treated as Completely Different\n",
            "======================================================================\n",
            "Vocabulary: ['adore', 'artificial', 'i', 'intelligence', 'learning', 'love', 'machine']\n",
            "\n",
            "'I love machine learning'\n",
            "‚Üí [0, 0, 1, 0, 1, 1, 1]\n",
            "\n",
            "'I adore artificial intelligence'\n",
            "‚Üí [1, 1, 1, 1, 0, 0, 0]\n",
            "\n",
            "==================================================\n",
            "Result: 'love' vs 'adore' and 'machine learning' vs 'AI' are completely different!\n",
            "But they have SIMILAR meanings!\n",
            "‚Üí BoW similarity = 0 even though meanings are related\n",
            "==================================================\n",
            "\n",
            "======================================================================\n",
            "Problem 3: Context is Lost\n",
            "======================================================================\n",
            "Vocabulary: ['bank', 'closed', 'is', 'muddy', 'river', 'the']\n",
            "\n",
            "'The bank is closed'\n",
            "‚Üí [1, 1, 1, 0, 0, 1]\n",
            "\n",
            "'The river bank is muddy'\n",
            "‚Üí [1, 0, 1, 1, 1, 1]\n",
            "\n",
            "==================================================\n",
            "Result: 'bank' appears in both, but means completely different things!\n",
            "‚Üí BoW can't distinguish context/meaning\n",
            "==================================================\n",
            "\n",
            "======================================================================\n",
            "SUMMARY: Syntactic Representation (BoW/TF-IDF) Limitations\n",
            "======================================================================\n",
            "‚ùå Loses word order\n",
            "‚ùå Can't handle synonyms (different words = 0 similarity)\n",
            "‚ùå Can't understand context\n",
            "‚ùå Only captures word counts, NOT meaning\n",
            "\n",
            "üìå CRITICAL: Syntactic ‚â† Semantic\n",
            "   - Syntactic (BOW/TF-IDF): Word-based, no meaning\n",
            "   - Semantic (Embeddings - Class 3): Meaning-based, understands synonyms\n",
            "\n",
            "‚úÖ But syntactic models are simple, interpretable, and work for many tasks!\n",
            "\n",
            "üí° Next class: We'll see embeddings (semantic representations) that solve these!\n",
            "   - Semantic = meaning - embeddings understand that 'space' and 'cosmic' are similar!\n"
          ]
        }
      ],
      "source": [
        "# Demonstrating BoW/Syntactic Representation Limitations\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Problem 1: Word Order is Lost\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Corpus: two documents with different word order\n",
        "docs_order = [\n",
        "    \"The dog bites the man\",\n",
        "    \"The man bites the dog\"\n",
        "]\n",
        "\n",
        "# Create BoW vectors for these two sentences\n",
        "# What do you notice about the vectors?\n",
        "# Build vocabulary from corpus\n",
        "all_words_order = set()\n",
        "for doc in docs_order:\n",
        "    tokens = preprocess_text(doc)\n",
        "    all_words_order.update(tokens)\n",
        "\n",
        "vocab_order = sorted(list(all_words_order))\n",
        "print(f\"Vocabulary: {vocab_order}\")\n",
        "\n",
        "bow_order = []\n",
        "for doc in docs_order:\n",
        "    tokens = preprocess_text(doc)\n",
        "    word_counts = Counter(tokens)\n",
        "    vector = [word_counts.get(word, 0) for word in vocab_order]\n",
        "    bow_order.append(vector)\n",
        "    print(f\"\\n'{doc}'\")\n",
        "    print(f\"‚Üí {vector}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Result: Both sentences have IDENTICAL BoW vectors!\")\n",
        "print(\"But they have COMPLETELY DIFFERENT meanings!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Problem 2: Synonyms are Treated as Completely Different\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Corpus: documents with synonyms\n",
        "docs_synonyms = [\n",
        "    \"I love machine learning\",\n",
        "    \"I adore artificial intelligence\"\n",
        "]\n",
        "\n",
        "# Create BoW vectors for these\n",
        "# Build vocabulary from corpus\n",
        "all_words_syn = set()\n",
        "for doc in docs_synonyms:\n",
        "    tokens = preprocess_text(doc)\n",
        "    all_words_syn.update(tokens)\n",
        "\n",
        "vocab_syn = sorted(list(all_words_syn))\n",
        "print(f\"Vocabulary: {vocab_syn}\")\n",
        "\n",
        "bow_syn = []\n",
        "for doc in docs_synonyms:\n",
        "    tokens = preprocess_text(doc)\n",
        "    word_counts = Counter(tokens)\n",
        "    vector = [word_counts.get(word, 0) for word in vocab_syn]\n",
        "    bow_syn.append(vector)\n",
        "    print(f\"\\n'{doc}'\")\n",
        "    print(f\"‚Üí {vector}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Result: 'love' vs 'adore' and 'machine learning' vs 'AI' are completely different!\")\n",
        "print(\"But they have SIMILAR meanings!\")\n",
        "print(\"‚Üí BoW similarity = 0 even though meanings are related\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Problem 3: Context is Lost\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Corpus: documents with same word but different context\n",
        "docs_context = [\n",
        "    \"The bank is closed\",      # Financial bank\n",
        "    \"The river bank is muddy\"  # River edge\n",
        "]\n",
        "\n",
        "# Build vocabulary from corpus\n",
        "all_words_ctx = set()\n",
        "for doc in docs_context:\n",
        "    tokens = preprocess_text(doc)\n",
        "    all_words_ctx.update(tokens)\n",
        "\n",
        "vocab_ctx = sorted(list(all_words_ctx))\n",
        "print(f\"Vocabulary: {vocab_ctx}\")\n",
        "\n",
        "bow_ctx = []\n",
        "for doc in docs_context:\n",
        "    tokens = preprocess_text(doc)\n",
        "    word_counts = Counter(tokens)\n",
        "    vector = [word_counts.get(word, 0) for word in vocab_ctx]\n",
        "    bow_ctx.append(vector)\n",
        "    print(f\"\\n'{doc}'\")\n",
        "    print(f\"‚Üí {vector}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Result: 'bank' appears in both, but means completely different things!\")\n",
        "print(\"‚Üí BoW can't distinguish context/meaning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY: Syntactic Representation (BoW/TF-IDF) Limitations\")\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ùå Loses word order\")\n",
        "print(\"‚ùå Can't handle synonyms (different words = 0 similarity)\")\n",
        "print(\"‚ùå Can't understand context\")\n",
        "print(\"‚ùå Only captures word counts, NOT meaning\")\n",
        "print(\"\\nüìå CRITICAL: Syntactic ‚â† Semantic\")\n",
        "print(\"   - Syntactic (BOW/TF-IDF): Word-based, no meaning\")\n",
        "print(\"   - Semantic (Embeddings - Class 3): Meaning-based, understands synonyms\")\n",
        "print(\"\\n‚úÖ But syntactic models are simple, interpretable, and work for many tasks!\")\n",
        "print(\"\\nüí° Next class: We'll see embeddings (semantic representations) that solve these!\")\n",
        "print(\"   - Semantic = meaning - embeddings understand that 'space' and 'cosmic' are similar!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vxjLc-jd762"
      },
      "source": [
        "---\n",
        "\n",
        "## End of Part 1: Foundation Complete! üéâ\n",
        "\n",
        "**Great job!** You've learned the fundamentals:\n",
        "- ‚úÖ **Keyword search** (simple and multiple keyword search)\n",
        "- ‚úÖ **Simple tokenization** (splitting text into words)\n",
        "- ‚úÖ **Text preprocessing** (cleaning, tokenization, regex)\n",
        "- ‚úÖ **Converting text to numbers** (Bag of Words - word counts/vectorization)\n",
        "- ‚úÖ **Understanding sparse vectors**\n",
        "- ‚úÖ **Recognizing BoW limitations** (syntactic, not semantic)\n",
        "\n",
        "**The Complete NLP Pipeline You Now Understand:**\n",
        "\n",
        "```\n",
        "Raw Text\n",
        "  ‚Üì\n",
        "1. Keyword Search (simple and multiple)\n",
        "  ‚Üì\n",
        "2. Simple Tokenization (split into words)\n",
        "  ‚Üì\n",
        "3. Preprocessing (clean, normalize)\n",
        "  ‚Üì\n",
        "4. Post-processing (filter stop words)\n",
        "  ‚Üì\n",
        "5. Vectorization (Bag of Words - word counts) ‚Üê You learned this!\n",
        "  ‚Üì\n",
        "6. TF-IDF (weighted vectors) ‚Üê Part 2\n",
        "  ‚Üì\n",
        "7. Similarity Search & Clustering ‚Üê Part 2\n",
        "```\n",
        "\n",
        "**Key Point**: **Bag of Words (BoW)** is just counting how many times each word appears in a document - simple word counts, not frequencies!\n",
        "\n",
        "**Now it's time to practice!** üèãÔ∏è\n",
        "\n",
        "Complete the exercises in the **Exercise Notebook** to reinforce what you've learned:\n",
        "- **Exercise 1**: Text cleaning with regex (URLs, emails, phone numbers)\n",
        "- **Exercise 2**: Tokenization with stop word removal\n",
        "- **Exercise 3**: Bag of Words implementation from scratch\n",
        "- **Exercise 4**: Keyword search implementation (simple and multiple)\n",
        "- **Exercise 5**: Similarity-based search with TF-IDF (still syntactic, not semantic!) - Part 2\n",
        "- **Exercise 6**: Document clustering with K-Means - Part 2\n",
        "- **Exercise 7**: Comparing preprocessing approaches\n",
        "- **Exercise 8**: Stemming and Lemmatization (advanced preprocessing)\n",
        "\n",
        "**Next in Part 2**: We'll learn TF-IDF (improving on BoW with weighted frequencies), similarity search, and clustering!\n",
        "\n",
        "Take a break, complete the exercises, then continue with **Learning Notebook Part 2** üëâ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}