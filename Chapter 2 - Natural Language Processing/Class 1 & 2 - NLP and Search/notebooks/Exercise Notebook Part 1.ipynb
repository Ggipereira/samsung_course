{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18c-5hetrJsF"
      },
      "source": [
        "# Exercise Notebook Part 1\n",
        "\n",
        "**Practice exercises for Part 1** - Follows **Learning Notebook Part 1**\n",
        "\n",
        "This notebook covers **Exercises 1-8** (foundational NLP preprocessing and basic search):\n",
        "\n",
        "1. **Exercise 1**: Text preprocessing with regex (URLs, emails, etc.) + sub-exercises (1a-1d)\n",
        "2. **Exercise 2**: Tokenization techniques comparison + sub-exercises (2a-2e)\n",
        "3. **Exercise 3**: Term Frequency (TF) calculation / Bag of Words (BoW)\n",
        "4. **Exercise 4**: TF-based keyword search\n",
        "5. **Exercise 5**: Compare preprocessing approaches\n",
        "6. **Exercise 6**: Stemming and Lemmatization + sub-exercises (6a-6d)\n",
        "7. **Exercise 7**: Advanced regex patterns + sub-exercises (7a-7d)\n",
        "8. **Exercise 8**: Handling special cases in preprocessing + sub-exercises (8a-8e)\n",
        "\n",
        "**üìù Note**: Exercise Notebook Part 2 will cover more advanced topics including TF-IDF, similarity search, document clustering, and RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "**Important Pipeline Order:**\n",
        "```\n",
        "Preprocessing ‚Üí Tokenization ‚Üí Vectorization (BoW/TF) ‚Üí Keyword Search\n",
        "```\n",
        "\n",
        "**Instructions**: Complete each exercise by filling in the code cells marked with `# TODO`\n",
        "\n",
        "**Note**:\n",
        "- **TF-IDF** (IDF calculation and full TF-IDF) is in **Exercise Notebook Part 2**\n",
        "- **Similarity search** and **RAG** are in **Exercise Notebook Part 2**\n",
        "- Remember that TF-IDF is **syntactic** (word-based, no meaning). True semantic search (understanding meaning, synonyms) requires embeddings (Class 3)! **Semantic = meaning**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bUUJ798urJsL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWih0ekOrJsM",
        "outputId": "78af7fd6-adfc-4fa8-ced3-2e4762ecb6ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file not found. Downloading from GitHub...\n",
            "‚úì Data file downloaded successfully!\n",
            "Loaded 10000 movies\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movie_id                title  \\\n",
              "0         1         Edge of Code   \n",
              "1         2      Storm of Secret   \n",
              "2         3  Under Warrior Redux   \n",
              "3         4      Quest of Secret   \n",
              "4         5          Key of Game   \n",
              "\n",
              "                                         description      genre  rating  \n",
              "0  A compelling romance film about a young advent...    Romance     7.1  \n",
              "1  This captivating romance movie follows a quest...    Romance     6.3  \n",
              "2  In this captivating war story, a secret organi...        War     7.3  \n",
              "3  A compelling fantasy film about a determined d...    Fantasy     8.3  \n",
              "4  A exploration adventure film about a master th...  Adventure     6.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e641f25-5e74-4fa8-b3d5-12776b0de829\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>genre</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Edge of Code</td>\n",
              "      <td>A compelling romance film about a young advent...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Storm of Secret</td>\n",
              "      <td>This captivating romance movie follows a quest...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Under Warrior Redux</td>\n",
              "      <td>In this captivating war story, a secret organi...</td>\n",
              "      <td>War</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Quest of Secret</td>\n",
              "      <td>A compelling fantasy film about a determined d...</td>\n",
              "      <td>Fantasy</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Key of Game</td>\n",
              "      <td>A exploration adventure film about a master th...</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e641f25-5e74-4fa8-b3d5-12776b0de829')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e641f25-5e74-4fa8-b3d5-12776b0de829 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e641f25-5e74-4fa8-b3d5-12776b0de829');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0e311cc3-c258-4829-a52c-939ba619b88d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e311cc3-c258-4829-a52c-939ba619b88d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0e311cc3-c258-4829-a52c-939ba619b88d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"movie_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 1,\n        \"max\": 10000,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6253,\n          4685,\n          1732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9983,\n        \"samples\": [\n          \"Agent of Hero\",\n          \"Battle Saga\",\n          \"Secret of Circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9136,\n        \"samples\": [\n          \"A engaging crime film about a secret organization. the film explores themes of courage and redemption.\",\n          \"A captivating western film about a forbidden love. featuring outstanding performances and breathtaking cinematography.\",\n          \"This thrilling action movie follows a small town. full of unexpected twists and turns. features spectacular action sequences and stunts.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"Romance\",\n          \"Sport\",\n          \"Western\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2023180604448267,\n        \"min\": 1.8,\n        \"max\": 10.0,\n        \"num_unique_values\": 79,\n        \"samples\": [\n          4.7,\n          7.1,\n          6.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Load movie data\n",
        "# If running in Google Colab and data file doesn't exist, download it from GitHub\n",
        "import os\n",
        "\n",
        "if not os.path.exists('data/movies.csv'):\n",
        "    print(\"Data file not found. Downloading from GitHub...\")\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    import urllib.request\n",
        "    url = 'https://raw.githubusercontent.com/samsung-ai-course/8th-9th-edition/main/Chapter%202%20-%20Natural%20Language%20Processing/Class%201%20%26%202%20-%20NLP%20and%20Search/data/movies.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/movies.csv')\n",
        "    print(\"‚úì Data file downloaded successfully!\")\n",
        "\n",
        "df = pd.read_csv('data/movies.csv')\n",
        "print(f\"Loaded {len(df)} movies\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVbxEAlCrJsN"
      },
      "source": [
        "## Exercise 1: Text Cleaning with Regex\n",
        "\n",
        "Complete the `clean_text` function to:\n",
        "1. Remove URLs (starting with http:// or https://)\n",
        "2. Remove email addresses\n",
        "3. Remove phone numbers (format: (555) 123-4567 or 555-123-4567)\n",
        "4. Remove extra whitespace\n",
        "5. Convert to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(sample_text):\n",
        "  text_no_urls = re.sub(r'https?://\\S+', '', sample_text)\n",
        "  text_no_emails = re.sub(r'\\S+@\\S+', '', text_no_urls)\n",
        "  text_no_phones = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '', text_no_emails)\n",
        "  text_clean = re.sub(r'[^\\w\\s]', ' ', text_no_phones)\n",
        "  text_final = re.sub(r'\\s+', ' ', text_clean).strip().lower()\n",
        "  return text_final"
      ],
      "metadata": {
        "id": "j_8Fd28Ys1xt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0h-6t48rJsO"
      },
      "source": [
        "### Exercise 1a: Extract Specific Patterns from Text\n",
        "\n",
        "Instead of removing patterns, sometimes we want to **extract** them for analysis.\n",
        "Complete functions to extract dates, prices, hashtags and anything else you might think its relevant.\n",
        "\n",
        "**Goal**: Practice pattern extraction and understand when to extract vs remove.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(sample_text):\n",
        "  text_no_urls = re.findall(r'https?://\\S+', '', sample_text)\n",
        "  text_no_emails = re.findall(r'\\S+@\\S+', '', text_no_urls)\n",
        "  text_no_phones = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '', text_no_emails)\n",
        "  text_clean = re.findall(r'[^\\w\\s]', ' ', text_no_phones)\n",
        "  text_final = re.findall(r'\\s+', ' ', text_clean).strip().lower()"
      ],
      "metadata": {
        "id": "pwTUxJZusm47"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sGpCTRqrJsO"
      },
      "source": [
        "### Exercise 1b: Normalize Text Content\n",
        "\n",
        "Sometimes we want to **normalize** text (standardize variations) rather than remove it.\n",
        "Complete functions to normalize contractions, abbreviations, and special characters.\n",
        "\n",
        "**Goal**: Understand when normalization improves text consistency for NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Text Normalization?\n",
        "\n",
        "**Normalization** in NLP means converting different variations of text into a **standard, consistent form** while preserving the meaning. Unlike **removal** (which deletes content), normalization **transforms** text to reduce variation and improve consistency.\n",
        "\n",
        "### Why Normalize?\n",
        "\n",
        "Text often contains multiple ways to express the same thing:\n",
        "- **Contractions**: \"don't\" vs \"do not\", \"I'm\" vs \"I am\", \"can't\" vs \"cannot\"\n",
        "- **Abbreviations**: \"Dr.\" vs \"Doctor\", \"U.S.A.\" vs \"USA\" vs \"United States\"\n",
        "- **Special characters**: \"caf√©\" vs \"cafe\", \"na√Øve\" vs \"naive\", \"r√©sum√©\" vs \"resume\"\n",
        "- **Punctuation variations**: \"Mr.\" vs \"Mr\", \"e.g.\" vs \"eg\"\n",
        "- **Number formats**: \"1,000\" vs \"1000\", \"50%\" vs \"50 percent\"\n",
        "\n",
        "### Normalization vs Removal\n",
        "\n",
        "| Approach | Example | When to Use |\n",
        "|----------|---------|-------------|\n",
        "| **Normalization** | \"don't\" ‚Üí \"do not\" | Preserve meaning, reduce vocabulary size |\n",
        "| **Removal** | Remove URLs, emails | Content is noise, not useful for analysis |\n",
        "\n",
        "### Benefits of Normalization\n",
        "\n",
        "1. **Reduces Vocabulary Size**: \"don't\", \"don't\", \"do not\", \"do not\" ‚Üí all become \"do not\"\n",
        "2. **Improves Matching**: Search for \"cannot\" will also match \"can't\"\n",
        "3. **Consistency**: Same concept represented the same way across documents\n",
        "4. **Better Statistics**: TF-IDF counts are more accurate when variations are unified\n",
        "\n",
        "### Examples of Normalization\n",
        "\n",
        "```python\n",
        "# Contractions\n",
        "\"don't\" ‚Üí \"do not\"\n",
        "\"I'm\" ‚Üí \"I am\"\n",
        "\"won't\" ‚Üí \"will not\"\n",
        "\"it's\" ‚Üí \"it is\" (or \"it has\" depending on context)\n",
        "\n",
        "# Abbreviations\n",
        "\"Dr. Smith\" ‚Üí \"Doctor Smith\"\n",
        "\"U.S.A.\" ‚Üí \"USA\" (or \"United States of America\")\n",
        "\"e.g.\" ‚Üí \"for example\"\n",
        "\"i.e.\" ‚Üí \"that is\"\n",
        "\n",
        "# Special Characters\n",
        "\"caf√©\" ‚Üí \"cafe\"\n",
        "\"na√Øve\" ‚Üí \"naive\"\n",
        "\"r√©sum√©\" ‚Üí \"resume\"\n",
        "\n",
        "# Punctuation\n",
        "\"Mr.\" ‚Üí \"Mr\"\n",
        "\"Mrs.\" ‚Üí \"Mrs\"\n",
        "\"Ph.D.\" ‚Üí \"PhD\"\n",
        "```\n",
        "\n",
        "### When NOT to Normalize\n",
        "\n",
        "- **Proper nouns**: \"U.S.A.\" (country) vs \"USA\" (abbreviation) - context matters\n",
        "- **Domain-specific terms**: \"AI\" vs \"artificial intelligence\" - may have different meanings\n",
        "- **Sentiment analysis**: \"don't\" vs \"do not\" - contractions can carry different emotional weight\n",
        "- **Preserving original format**: When exact text matching is required\n",
        "\n",
        "### Implementation Strategy\n",
        "\n",
        "1. **Contractions**: Use a dictionary mapping contractions to full forms\n",
        "2. **Abbreviations**: Create abbreviation dictionaries (context-dependent)\n",
        "3. **Special Characters**: Use Unicode normalization (NFD/NFC) or character mapping\n",
        "4. **Punctuation**: Remove or standardize punctuation marks consistently\n",
        "\n",
        "**Key Insight**: Normalization is a trade-off between consistency and information preservation. Choose normalization strategies based on your specific NLP task!\n",
        "\n",
        "---\n",
        "\n",
        "### üí° PS: Useful Frameworks for Normalization\n",
        "\n",
        "Instead of manually creating endless dictionaries for contractions, abbreviations, and special characters, consider using established NLP frameworks:\n",
        "\n",
        "- **spaCy**: Provides built-in text normalization, lemmatization, and tokenization\n",
        "  ```python\n",
        "  import spacy\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(\"I don't think it's working\")\n",
        "  # Access normalized tokens, lemmas, etc.\n",
        "  ```\n",
        "\n",
        "- **NLTK**: Offers contraction expansion, word normalization, and various text processing utilities\n",
        "  ```python\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  from nltk.corpus import stopwords\n",
        "  ```\n",
        "\n",
        "- **TextBlob**: Simple API for common NLP tasks including normalization\n",
        "  ```python\n",
        "  from textblob import TextBlob\n",
        "  blob = TextBlob(\"I don't like it\")\n",
        "  ```\n",
        "\n",
        "- **Unidecode**: Specifically for Unicode normalization (removing accents, special characters)\n",
        "  ```python\n",
        "  from unidecode import unidecode\n",
        "  unidecode(\"caf√©\")  # Returns \"cafe\"\n",
        "  ```\n",
        "\n",
        "- **contractions**: Python library specifically for expanding contractions\n",
        "  ```python\n",
        "  import contractions\n",
        "  contractions.fix(\"don't\")  # Returns \"do not\"\n",
        "  ```\n",
        "\n",
        "**Note**: While these frameworks are helpful, understanding the underlying concepts (as you'll practice in this exercise) is crucial for customizing normalization for your specific use case!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0DTxS8rJsP"
      },
      "source": [
        "### Exercise 1c: Clean HTML and Markdown\n",
        "\n",
        "Real-world text often contains HTML tags or markdown formatting.\n",
        "Complete functions to remove HTML tags while preserving text content, and clean markdown.\n",
        "\n",
        "**Goal**: Handle structured text formats commonly found in web content.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(text):\n",
        "    # 1) Remover blocos <script>...</script> e <style>...</style> (n√£o interessam para NLP)\n",
        "    text = re.sub(r'<script.*?>.*?</script>', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    text = re.sub(r'<style.*?>.*?</style>', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # 2) Remover todas as restantes tags HTML: <...>\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # 3) Normalizar espa√ßos (j√° conheces :) )\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "1UGiw87Iva-L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxWSDzqNrJsQ"
      },
      "source": [
        "### Exercise 1d: Compare Cleaning Strategies\n",
        "\n",
        "Compare the impact of different cleaning approaches on vocabulary size and text quality.\n",
        "This helps understand when to apply different cleaning techniques.\n",
        "\n",
        "**Goal**: Measure the practical impact of preprocessing choices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9lBEAJH3rJsQ",
        "outputId": "0a1e7e7c-e903-4f55-a2fc-19dd761a9089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for &: 'str' and 'RegexFlag'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1849584220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Run comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mcomparison_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_cleaning_strategies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1849584220.py\u001b[0m in \u001b[0;36mcompare_cleaning_strategies\u001b[0;34m(df, column, sample_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# TODO: Apply full cleaning (remove URLs, emails, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use your function from Exercise 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mvocab_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3182022569.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(sample_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtext_no_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'https?://\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtext_no_emails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\S+@\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_no_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtext_no_phones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_no_emails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtext_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_no_phones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/re/__init__.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/re/__init__.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             warnings.warn(\"The re.TEMPLATE/re.T flag is deprecated \"\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'str' and 'RegexFlag'"
          ]
        }
      ],
      "source": [
        "def compare_cleaning_strategies(df, column='description', sample_size=10):\n",
        "    \"\"\"\n",
        "    Compare different cleaning strategies and measure their impact.\n",
        "\n",
        "    Strategies to compare:\n",
        "    1. No cleaning (baseline)\n",
        "    2. Basic cleaning (lowercase, whitespace)\n",
        "    3. Full cleaning (remove URLs, emails, etc.)\n",
        "    4. Full cleaning + normalization (contractions, abbreviations)\n",
        "\n",
        "    Metrics to measure:\n",
        "    - Vocabulary size (unique words)\n",
        "    - Average document length\n",
        "    - Number of tokens\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    sample_texts = df[column].head(sample_size).tolist()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Strategy 1: No cleaning\n",
        "    vocab_no_clean = set()\n",
        "    total_tokens_no_clean = 0\n",
        "    for text in sample_texts:\n",
        "        tokens = text.split()\n",
        "        vocab_no_clean.update(tokens)\n",
        "        total_tokens_no_clean += len(tokens)\n",
        "\n",
        "    results['No Cleaning'] = {\n",
        "        'vocab_size': len(vocab_no_clean),\n",
        "        'total_tokens': total_tokens_no_clean,\n",
        "        'avg_length': total_tokens_no_clean / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 2: Basic cleaning (lowercase, whitespace)\n",
        "    vocab_basic = set()\n",
        "    total_tokens_basic = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply basic cleaning (lowercase, normalize whitespace)\n",
        "        cleaned = text.lower()\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "        tokens = cleaned.split()\n",
        "        vocab_basic.update(tokens)\n",
        "        total_tokens_basic += len(tokens)\n",
        "\n",
        "    results['Basic Cleaning'] = {\n",
        "        'vocab_size': len(vocab_basic),\n",
        "        'total_tokens': total_tokens_basic,\n",
        "        'avg_length': total_tokens_basic / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 3: Full cleaning (use your clean_text function)\n",
        "    vocab_full = set()\n",
        "    total_tokens_full = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply full cleaning (remove URLs, emails, etc.)\n",
        "        cleaned = clean_text(text)  # Use your function from Exercise 1\n",
        "        tokens = cleaned.split()\n",
        "        vocab_full.update(tokens)\n",
        "        total_tokens_full += len(tokens)\n",
        "\n",
        "    results['Full Cleaning'] = {\n",
        "        'vocab_size': len(vocab_full),\n",
        "        'total_tokens': total_tokens_full,\n",
        "        'avg_length': total_tokens_full / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 4: Full cleaning + normalization\n",
        "    vocab_norm = set()\n",
        "    total_tokens_norm = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply full cleaning + normalization\n",
        "        cleaned = clean_text(text)\n",
        "        cleaned = normalize_contractions(cleaned)\n",
        "        cleaned = normalize_abbreviations(cleaned)\n",
        "        tokens = cleaned.split()\n",
        "        vocab_norm.update(tokens)\n",
        "        total_tokens_norm += len(tokens)\n",
        "\n",
        "    results['Full + Normalization'] = {\n",
        "        'vocab_size': len(vocab_norm),\n",
        "        'total_tokens': total_tokens_norm,\n",
        "        'avg_length': total_tokens_norm / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Display comparison\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Cleaning Strategy Comparison\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Strategy':<25} {'Vocab Size':<15} {'Total Tokens':<15} {'Avg Length':<15}\")\n",
        "    print(\"-\" * 70)\n",
        "    for strategy, metrics in results.items():\n",
        "        print(f\"{strategy:<25} {metrics['vocab_size']:<15} {metrics['total_tokens']:<15} {metrics['avg_length']:<15.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Key Insights:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Vocabulary reduction: {results['No Cleaning']['vocab_size']} ‚Üí {results['Full + Normalization']['vocab_size']}\")\n",
        "    print(f\"Reduction percentage: {(1 - results['Full + Normalization']['vocab_size'] / results['No Cleaning']['vocab_size']) * 100:.1f}%\")\n",
        "    print(\"\\nWhen to use each strategy:\")\n",
        "    print(\"  - No cleaning: Preserve original text for exact matching\")\n",
        "    print(\"  - Basic cleaning: Simple normalization, fast processing\")\n",
        "    print(\"  - Full cleaning: Remove noise, reduce vocabulary size\")\n",
        "    print(\"  - Full + Normalization: Maximum consistency, best for NLP tasks\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run comparison\n",
        "comparison_results = compare_cleaning_strategies(df, sample_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzrTi8KYrJsR"
      },
      "source": [
        "## Exercise 2: Tokenization\n",
        "\n",
        "Complete the `tokenize_text` function to:\n",
        "1. Tokenize text into words\n",
        "2. Filter out very short tokens (length < 3)\n",
        "3. Optionally filter stop words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  tokens_regex = re.findall(r'\\w+', text.lower())\n",
        "  tokens_clean= [token for token in tokens_regex if len(token)>3]\n",
        "  return tokens_clean"
      ],
      "metadata": {
        "id": "Ia43R6IXwvG8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase=\"ola o meu nome √© JOAO e gosto de jogar a boLa\"\n",
        "tokenize_text(text=frase)"
      ],
      "metadata": {
        "id": "YgsRHhehxeIr",
        "outputId": "0b81f616-9645-4172-a955-8ff4a32d6f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nome', 'joao', 'gosto', 'jogar', 'bola']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UwXZZZVrJsS"
      },
      "source": [
        "### Exercise 2a: Compare Tokenization Methods\n",
        "\n",
        "Compare different tokenization approaches to understand their impact.\n",
        "Different methods split text differently, affecting vocabulary size and token quality.\n",
        "\n",
        "**Goal**: Understand trade-offs between different tokenization techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoBP6MNVxsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldxkq3n5rJsS"
      },
      "source": [
        "### Exercise 2b: Test Impact of min_length Threshold\n",
        "\n",
        "Test how different minimum token length thresholds affect vocabulary size and token quality.\n",
        "Shorter tokens (like \"a\", \"I\", \"it\") are often less informative but may be important in some contexts.\n",
        "\n",
        "**Goal**: Understand the trade-off between vocabulary size and token informativeness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th9lc50hrJsS"
      },
      "source": [
        "### Exercise 2c: Test Impact of Stop Word Removal\n",
        "\n",
        "Compare tokenization with and without stop word removal.\n",
        "Stop words are common but may be important for some tasks (e.g., sentiment analysis).\n",
        "\n",
        "**Goal**: Understand when stop word removal helps vs. hurts NLP tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCJcxG17yhNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XEyK8f6rJsS"
      },
      "source": [
        "### Exercise 2d: Tokenize Different Text Types\n",
        "\n",
        "Compare tokenization on different types of text (titles vs descriptions).\n",
        "Different text types have different characteristics and may need different preprocessing.\n",
        "\n",
        "**Goal**: Understand how text type affects tokenization decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdz1Mc51rJsT"
      },
      "source": [
        "### Exercise 2e: Create N-grams from Tokens\n",
        "\n",
        "After tokenization, create n-grams (unigrams, bigrams, trigrams) to capture word order.\n",
        "Compare vocabulary size and sparsity of different n-gram approaches.\n",
        "\n",
        "**Goal**: Understand how n-grams capture context and affect vocabulary size.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_regex = re.findall(r'\\w+', text.lower())\n",
        "tokens_clean= [token for token in tokens_regex if len(token)>3]\n",
        "return tokens_clean\n",
        "def create_ngrams(tokens, n=2):\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        ngrams = []\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            # Create n-gram by joining n consecutive tokens\n",
        "            ngram = ' '.join(tokens[i:i+n])\n",
        "            ngrams.append(ngram)\n",
        "        return ngrams"
      ],
      "metadata": {
        "id": "nv1AuXpVytU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv62qJVQrJsT"
      },
      "source": [
        "## Exercise 3: Bag of Words (BoW) / Term Frequency (TF) Calculation\n",
        "\n",
        "Implement Term Frequency (TF) calculation manually - this is also called Bag of Words (BoW)!\n",
        "\n",
        "**Note**: This is Part 1 of vectorization. In **Exercise Notebook Part 2**, you'll learn IDF and full TF-IDF calculation.\n",
        "\n",
        "**What you'll implement:**\n",
        "1. Calculate Term Frequency (TF) - count how many times a word appears in a document\n",
        "2. TF = count(term) / total_terms_in_document\n",
        "3. This creates Bag of Words vectors - simple word counts!\n",
        "\n",
        "**Key Point**: BoW/TF is just counting words - it's the foundation before we learn TF-IDF in Part 2!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "776qpmGTrJsT",
        "outputId": "1290b87a-758b-4fc0-c998-cbef04ceacc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['deep', 'language', 'learning', 'machine', 'natural', 'processing']\n",
            "\n",
            "TF of 'natural' in doc 0: 0.3333333333333333\n",
            "TF of 'language' in doc 0: 0.3333333333333333\n",
            "\n",
            "Bag of Words vectors:\n",
            "Doc 0: ['natural', 'language', 'processing'] ‚Üí [0, 1, 0, 0, 1, 1]\n",
            "Doc 1: ['machine', 'learning', 'natural'] ‚Üí [0, 0, 1, 1, 1, 0]\n",
            "Doc 2: ['deep', 'learning', 'language'] ‚Üí [1, 1, 1, 0, 0, 0]\n",
            "\n",
            "üí° Note: These are simple word counts (BoW). In Part 2, you'll learn TF-IDF!\n"
          ]
        }
      ],
      "source": [
        "def calculate_tf(term, document_tokens):\n",
        "    \"\"\"\n",
        "    Calculate Term Frequency: count(term) / total_terms\n",
        "\n",
        "    Note: This is also called Bag of Words (BoW) - BoW = TF!\n",
        "\n",
        "    Args:\n",
        "        term (str): The word to count\n",
        "        document_tokens (list): List of tokens in the document\n",
        "\n",
        "    Returns:\n",
        "        float: Term frequency\n",
        "    \"\"\"\n",
        "    # TODO: Count how many times term appears in document_tokens\n",
        "    times=document_tokens.count(term)\n",
        "    # TODO: Divide by total number of tokens\n",
        "    return times/len(document_tokens)\n",
        "\n",
        "def create_bow_vector(document_tokens, vocabulary):\n",
        "    \"\"\"\n",
        "    Create a Bag of Words vector for a document.\n",
        "\n",
        "    Args:\n",
        "        document_tokens (list): List of tokens in the document\n",
        "        vocabulary (list): List of all unique words in the corpus (sorted)\n",
        "\n",
        "    Returns:\n",
        "        list: BoW vector where each element is the count of that word in the document\n",
        "    \"\"\"\n",
        "\n",
        "    token_counts = Counter(document_tokens)\n",
        "\n",
        "\n",
        "    bow_vector = [token_counts.get(word, 0) for word in vocabulary]\n",
        "\n",
        "    return bow_vector\n",
        "\n",
        "# Test with simple example\n",
        "docs = [\n",
        "    [\"natural\", \"language\", \"processing\"],\n",
        "    [\"machine\", \"learning\", \"natural\"],\n",
        "    [\"deep\", \"learning\", \"language\"]\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "all_words = set()\n",
        "for doc in docs:\n",
        "    all_words.update(doc)\n",
        "vocab = sorted(list(all_words))\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nTF of 'natural' in doc 0:\", calculate_tf(\"natural\", docs[0]))\n",
        "print(\"TF of 'language' in doc 0:\", calculate_tf(\"language\", docs[0]))\n",
        "\n",
        "print(\"\\nBag of Words vectors:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    bow_vector = create_bow_vector(doc, vocab)\n",
        "    print(f\"Doc {i}: {doc} ‚Üí {bow_vector}\")\n",
        "\n",
        "print(\"\\nüí° Note: These are simple word counts (BoW). In Part 2, you'll learn TF-IDF!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4tzOTq2rJsU"
      },
      "source": [
        "## Exercise 4: TF-Based Keyword Search\n",
        "\n",
        "Implement a keyword search that ranks results by Term Frequency (TF).\n",
        "For multiple query words, combine their TF scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_by_tf(query_terms, documents_tokens):\n",
        "    results = []\n",
        "\n",
        "    for i, doc_tokens in enumerate(documents_tokens):\n",
        "        score = 0.0\n",
        "        # somar TF de cada termo da query neste documento\n",
        "        for term in query_terms:\n",
        "            score += calculate_tf(term, doc_tokens)\n",
        "        results.append((i, score))\n",
        "\n",
        "    # ordenar por score TF (maior primeiro)\n",
        "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    return results_sorted"
      ],
      "metadata": {
        "id": "sKBb5rqh6kPC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HQ6f1rrrJsU"
      },
      "source": [
        "## Exercise 5: Compare Preprocessing Approaches\n",
        "\n",
        "Compare search results with and without preprocessing (stop words removal, lowercasing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZC5MmSOrJsU"
      },
      "source": [
        "## Exercise 6: Stemming and Lemmatization\n",
        "\n",
        "Stemming and lemmatization are advanced preprocessing techniques that reduce words to their root forms:\n",
        "- **Stemming**: Quick, rule-based reduction (e.g., \"running\" ‚Üí \"run\", \"better\" ‚Üí \"better\")\n",
        "- **Lemmatization**: More accurate, context-aware reduction using dictionaries (e.g., \"better\" ‚Üí \"good\")\n",
        "\n",
        "**When to use**: Useful for reducing vocabulary size and handling word variations (running/ran/run).\n",
        "\n",
        "Complete exercises below to understand both approaches!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QR69AG1rJsU"
      },
      "source": [
        "### Exercise 6a: Implement Stemming\n",
        "\n",
        "Complete the `apply_stemming` function using NLTK's PorterStemmer or SnowballStemmer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()          # <--- ISTO √â O IMPORTANTE\n",
        "lemmatizer = WordNetLemmatizer()   # (para a parte da lemmatization)"
      ],
      "metadata": {
        "id": "kRoYu3sd7noF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "U8uGvmgw78IH",
        "outputId": "a411b1b8-7cc6-4684-cc4f-7ab01a3f4e63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_stemming(tokens):\n",
        "  return [stemmer.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "L_RQQ-_U68ix"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnLCao0rJsV"
      },
      "source": [
        "### Exercise 6b: Implement Lemmatization\n",
        "\n",
        "Complete the `apply_lemmatization` function using NLTK's WordNetLemmatizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lemmatization(tokens):\n",
        "  return [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "mWHGhgd07RZX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj8kK_vjrJsV"
      },
      "source": [
        "### Exercise 6c: Compare Stemming vs Lemmatization\n",
        "\n",
        "Compare the results of stemming and lemmatization on the same text.\n",
        "Analyze when each approach is better.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cats were running and the mice were better at studies\"\n",
        "tokens = sentence.lower().split()\n",
        "\n",
        "print(\"Original tokens:      \", tokens)\n",
        "print(\"Stemming:             \", apply_stemming(tokens))\n",
        "print(\"Lemmatization (simple)\", apply_lemmatization(tokens))\n"
      ],
      "metadata": {
        "id": "m7XoXYvs7WdF",
        "outputId": "4794e53c-d688-4360-a245-b165388068b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens:       ['the', 'cats', 'were', 'running', 'and', 'the', 'mice', 'were', 'better', 'at', 'studies']\n",
            "Stemming:              ['the', 'cat', 'were', 'run', 'and', 'the', 'mice', 'were', 'better', 'at', 'studi']\n",
            "Lemmatization (simple) ['the', 'cat', 'were', 'running', 'and', 'the', 'mouse', 'were', 'better', 'at', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj8x5KbqrJsV"
      },
      "source": [
        "### Exercise 6d: Complete Preprocessing Pipeline with Stemming/Lemmatization\n",
        "\n",
        "Create a complete preprocessing function that includes optional stemming or lemmatization.\n",
        "Compare search results with and without stemming/lemmatization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PWSfVbJIrJsV",
        "outputId": "27725652-87ba-4915-d4a1-25ed26400ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original description: A compelling romance film about a young adventurer. an epic adventure that spans continents and generations. a touching love story that will warm your heart.\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text_complete(text,\n",
        "                             remove_stop_words=True,\n",
        "                             min_length=3,\n",
        "                             stem=False,\n",
        "                             lemmatize=False):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline with optional stemming/lemmatization.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text input\n",
        "        remove_stop_words: Whether to remove stop words\n",
        "        min_length: Minimum token length\n",
        "        stem: If True, apply stemming\n",
        "        lemmatize: If True, apply lemmatization (overrides stem if both True)\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed tokens\n",
        "    \"\"\"\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    # Step 1: Clean text (use your clean_text function from Exercise 1)\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Step 2: Tokenize (use your tokenize_text or word_tokenize)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Step 3: Remove stop words (if enabled)\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # TODO: Filter out stop words\n",
        "\n",
        "    # Step 4: Filter by length\n",
        "    # TODO: Keep only tokens with length >= min_length\n",
        "\n",
        "    # Step 5: Apply stemming or lemmatization\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        # TODO: Apply lemmatization\n",
        "    elif stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        # TODO: Apply stemming\n",
        "\n",
        "    return []\n",
        "\n",
        "# Test on a movie description\n",
        "sample_desc = df.loc[0, 'description']\n",
        "print(\"Original description:\", sample_desc[:200])\n",
        "\n",
        "# TODO: Test different preprocessing combinations\n",
        "# tokens_basic = preprocess_text_complete(sample_desc, stem=False, lemmatize=False)\n",
        "# tokens_stemmed = preprocess_text_complete(sample_desc, stem=True, lemmatize=False)\n",
        "# tokens_lemmatized = preprocess_text_complete(sample_desc, stem=False, lemmatize=True)\n",
        "\n",
        "# TODO: Compare vocabulary size reduction\n",
        "# TODO: Compare search results with different preprocessing approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsExpe-XrJsW"
      },
      "source": [
        "## Summary: What You've Practiced in Part 1\n",
        "\n",
        "‚úÖ **Exercise 1**: Text cleaning with regex (URLs, emails, phone numbers)  \n",
        "  - **1a**: Extract specific patterns (dates, prices, hashtags, mentions)  \n",
        "  - **1b**: Normalize text content (contractions, abbreviations, special chars)  \n",
        "  - **1c**: Clean HTML and Markdown  \n",
        "  - **1d**: Compare cleaning strategies  \n",
        "\n",
        "‚úÖ **Exercise 2**: Tokenization with stop word removal  \n",
        "  - **2a**: Compare tokenization methods (split, regex, word boundaries)  \n",
        "  - **2b**: Test impact of min_length threshold  \n",
        "  - **2c**: Test impact of stop word removal  \n",
        "  - **2d**: Tokenize different text types (titles vs descriptions)  \n",
        "  - **2e**: Create n-grams from tokens  \n",
        "\n",
        "‚úÖ **Exercise 3**: Bag of Words (BoW) / Term Frequency (TF) calculation  \n",
        "‚úÖ **Exercise 4**: TF-based keyword search  \n",
        "‚úÖ **Exercise 5**: Compare preprocessing approaches  \n",
        "‚úÖ **Exercise 6**: Stemming and Lemmatization (6a, 6b, 6c, 6d)  \n",
        "‚úÖ **Exercise 7**: Advanced regex patterns  \n",
        "‚úÖ **Exercise 8**: Handling special cases in preprocessing  \n",
        "\n",
        "**Key Takeaways:**\n",
        "- Preprocessing quality directly affects search results\n",
        "- Different tokenization methods have different trade-offs (speed vs. accuracy vs. context)\n",
        "- min_length and stop word removal significantly impact vocabulary size\n",
        "- Text type matters: titles vs descriptions need different approaches\n",
        "- N-grams capture word order but increase vocabulary size and sparsity\n",
        "- Stemming vs Lemmatization: Choose based on speed vs accuracy needs\n",
        "- Regex is powerful for cleaning but handle edge cases carefully\n",
        "- Bag of Words (BoW/TF) is simple word counting - foundation for TF-IDF!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp9OLpSArJsX"
      },
      "source": [
        "## Exercise 7: Advanced Regex Patterns\n",
        "\n",
        "Practice more complex regex patterns for text preprocessing.\n",
        "\n",
        "**Goal**: Master advanced regex techniques commonly used in NLP preprocessing.\n",
        "\n",
        "**What you'll practice:**\n",
        "1. Named groups and non-capturing groups\n",
        "2. Lookahead and lookbehind assertions\n",
        "3. Complex pattern matching (dates, currencies, numbers)\n",
        "4. Regex substitution with callbacks\n",
        "5. Handling unicode and special characters\n",
        "\n",
        "**Why this matters**: Real-world text contains complex patterns that require sophisticated regex to extract or clean properly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmM9p9JlrJsX"
      },
      "source": [
        "# Exercise 7a: Extract Dates in Different Formats\n",
        "\n",
        "def extract_dates(text):\n",
        "    \"\"\"\n",
        "    Extract dates in various formats:\n",
        "    - \"January 1, 2024\" or \"Jan 1, 2024\"\n",
        "    - \"2024-01-01\" or \"01/01/2024\"\n",
        "    - \"1st January 2024\"\n",
        "    \n",
        "    Use named groups to extract day, month, year separately.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text containing dates\n",
        "    \n",
        "    Returns:\n",
        "        list: List of tuples (day, month, year) or dicts with named groups\n",
        "    \"\"\"\n",
        "    # TODO: Create regex pattern(s) to match different date formats\n",
        "    # TODO: Use named groups (?P<name>pattern) to extract components\n",
        "    # TODO: Return list of matches\n",
        "    \n",
        "    dates = []\n",
        "    # Hint: Use re.finditer() or re.findall() with named groups\n",
        "    \n",
        "    return dates\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie was released on January 15, 2024.\n",
        "    It premiered on 2024-03-20 in theaters.\n",
        "    The sequel came out on 04/15/2024.\n",
        "    The original was on 1st January 2020.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract dates and print results\n",
        "# dates = extract_dates(test_text)\n",
        "# for date in dates:\n",
        "#     print(date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcbJ-3XXrJsY"
      },
      "outputs": [],
      "source": [
        "# Exercise 7b: Extract Currency and Numbers\n",
        "\n",
        "def extract_currency(text):\n",
        "    \"\"\"\n",
        "    Extract currency amounts in different formats:\n",
        "    - \"$100\", \"$1,000.50\", \"$1M\", \"$1.5B\"\n",
        "    - \"‚Ç¨50\", \"¬£200\", \"¬•1000\"\n",
        "    - \"100 dollars\", \"fifty euros\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        list: List of currency amounts with their symbols/units\n",
        "    \"\"\"\n",
        "    # TODO: Create regex patterns to match currency formats\n",
        "    # TODO: Extract amount and currency symbol/unit\n",
        "    # TODO: Handle different currency symbols ($, ‚Ç¨, ¬£, ¬•)\n",
        "    # TODO: Handle abbreviations (M = million, B = billion, K = thousand)\n",
        "\n",
        "    currencies = []\n",
        "\n",
        "    return currencies\n",
        "\n",
        "def extract_numbers(text):\n",
        "    \"\"\"\n",
        "    Extract numbers in various formats:\n",
        "    - Integers: \"100\", \"1,000\", \"1 million\"\n",
        "    - Decimals: \"3.14\", \"1,234.56\"\n",
        "    - Percentages: \"50%\", \"25.5 percent\"\n",
        "    - Ordinals: \"1st\", \"2nd\", \"3rd\", \"4th\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with keys 'integers', 'decimals', 'percentages', 'ordinals'\n",
        "    \"\"\"\n",
        "    # TODO: Extract different number types\n",
        "    # TODO: Use named groups or separate patterns for each type\n",
        "\n",
        "    numbers = {\n",
        "        'integers': [],\n",
        "        'decimals': [],\n",
        "        'percentages': [],\n",
        "        'ordinals': []\n",
        "    }\n",
        "\n",
        "    return numbers\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie grossed $150 million at the box office.\n",
        "    It cost $50M to produce and made ‚Ç¨75.5M worldwide.\n",
        "    The rating was 8.5/10 with 95% positive reviews.\n",
        "    It ranked 1st in its opening weekend, 2nd overall.\n",
        "    The budget was approximately 1.5 billion dollars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract currencies and numbers\n",
        "# currencies = extract_currency(test_text)\n",
        "# numbers = extract_numbers(test_text)\n",
        "# print(\"Currencies:\", currencies)\n",
        "# print(\"Numbers:\", numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da0_X56brJsY"
      },
      "outputs": [],
      "source": [
        "# Exercise 7c: Lookahead and Lookbehind Patterns\n",
        "\n",
        "def extract_quoted_text(text):\n",
        "    \"\"\"\n",
        "    Extract text within quotes, handling nested quotes.\n",
        "    Use lookahead/lookbehind to ensure proper matching.\n",
        "\n",
        "    Args:\n",
        "        text: Input text with quoted strings\n",
        "\n",
        "    Returns:\n",
        "        list: List of quoted text (without the quotes)\n",
        "    \"\"\"\n",
        "    # TODO: Use positive lookbehind (?<=...) and positive lookahead (?=...)\n",
        "    # TODO: Match text between quotes (single or double)\n",
        "    # TODO: Handle escaped quotes inside strings\n",
        "\n",
        "    quoted = []\n",
        "\n",
        "    return quoted\n",
        "\n",
        "def extract_context_words(text, target_word, context_size=3):\n",
        "    \"\"\"\n",
        "    Extract words around a target word using lookahead/lookbehind.\n",
        "\n",
        "    Example: For \"machine learning\" in \"I love machine learning and AI\",\n",
        "             extract \"love\", \"and\", \"AI\" (context_size=1)\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        target_word: Word to find context for\n",
        "        context_size: Number of words before and after to extract\n",
        "\n",
        "    Returns:\n",
        "        dict: {'before': [words], 'after': [words], 'target': word}\n",
        "    \"\"\"\n",
        "    # TODO: Use lookbehind to capture words before target\n",
        "    # TODO: Use lookahead to capture words after target\n",
        "    # TODO: Return context words\n",
        "\n",
        "    context = {\n",
        "        'before': [],\n",
        "        'after': [],\n",
        "        'target': target_word\n",
        "    }\n",
        "\n",
        "    return context\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The director said \"This is the best movie I've ever made.\"\n",
        "    He added, 'It's a masterpiece' and everyone agreed.\n",
        "    The phrase \"machine learning\" appears often in AI discussions.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract quoted text and context\n",
        "# quoted = extract_quoted_text(test_text)\n",
        "# context = extract_context_words(test_text, \"machine\", context_size=2)\n",
        "# print(\"Quoted text:\", quoted)\n",
        "# print(\"Context:\", context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFYRv3fqrJsY"
      },
      "outputs": [],
      "source": [
        "# Exercise 7d: Regex Substitution with Callbacks\n",
        "\n",
        "def anonymize_emails(text):\n",
        "    \"\"\"\n",
        "    Replace email addresses with \"[EMAIL]\" placeholder.\n",
        "    Use re.sub() with a function callback.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with emails anonymized\n",
        "    \"\"\"\n",
        "    # TODO: Use re.sub() with a function that replaces email pattern\n",
        "    # Pattern: something@domain.com\n",
        "    # Replace with: \"[EMAIL]\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def normalize_whitespace_advanced(text):\n",
        "    \"\"\"\n",
        "    Normalize whitespace, but preserve intentional line breaks.\n",
        "    - Replace multiple spaces with single space\n",
        "    - Replace multiple newlines (2+) with double newline (paragraph break)\n",
        "    - Preserve single newlines (line breaks)\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    # TODO: Use re.sub() with callbacks to handle different whitespace patterns\n",
        "    # TODO: Preserve intentional formatting while cleaning up excessive whitespace\n",
        "\n",
        "    return text\n",
        "\n",
        "def format_numbers_readable(text):\n",
        "    \"\"\"\n",
        "    Format large numbers to be more readable.\n",
        "    - \"1000000\" ‚Üí \"1,000,000\"\n",
        "    - \"1500\" ‚Üí \"1,500\"\n",
        "    - But preserve decimals: \"1234.56\" ‚Üí \"1,234.56\"\n",
        "\n",
        "    Use re.sub() with a callback function to format numbers.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with formatted numbers\n",
        "    \"\"\"\n",
        "    # TODO: Find numbers in text\n",
        "    # TODO: Use callback function to add commas every 3 digits\n",
        "    # TODO: Preserve decimal points\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    Contact us at info@example.com or support@company.org.\n",
        "    The movie made 1500000 dollars in its first week.\n",
        "    It had 5000 viewers on opening day.\n",
        "    The budget was 2500000.50 dollars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test anonymization and formatting\n",
        "# anonymized = anonymize_emails(test_text)\n",
        "# formatted = format_numbers_readable(test_text)\n",
        "# print(\"Anonymized:\", anonymized)\n",
        "# print(\"Formatted:\", formatted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0eKkBrDrJsZ"
      },
      "source": [
        "## Exercise 8: Handling Special Cases in Preprocessing\n",
        "\n",
        "Handle edge cases in text preprocessing that are common in real-world data.\n",
        "\n",
        "**Goal**: Make preprocessing robust to handle messy, real-world text data.\n",
        "\n",
        "**What you'll handle:**\n",
        "1. Mixed encoding issues (unicode, emojis, special characters)\n",
        "2. Inconsistent capitalization (acronyms, proper nouns)\n",
        "3. Numbers and units (measurements, percentages, dates)\n",
        "4. Abbreviations and contractions\n",
        "5. Whitespace inconsistencies and formatting artifacts\n",
        "6. Missing or corrupted data (NaN, None, empty strings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InIn1ofUrJsb"
      },
      "outputs": [],
      "source": [
        "# Exercise 6a: Handle Unicode and Special Characters\n",
        "\n",
        "def clean_unicode_text(text):\n",
        "    \"\"\"\n",
        "    Clean text with unicode issues:\n",
        "    - Normalize unicode characters (√© ‚Üí e, √± ‚Üí n)\n",
        "    - Remove or replace emojis\n",
        "    - Handle special quote characters (\"\" ‚Üí \", '' ‚Üí ')\n",
        "    - Remove zero-width spaces and other invisible characters\n",
        "\n",
        "    Args:\n",
        "        text: Input text with unicode issues\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    import unicodedata\n",
        "\n",
        "    # TODO: Normalize unicode (NFD or NFC)\n",
        "    # TODO: Remove or replace emojis\n",
        "    # TODO: Normalize quotes and special characters\n",
        "    # TODO: Remove invisible characters\n",
        "\n",
        "    return text\n",
        "\n",
        "def handle_mixed_encoding(text):\n",
        "    \"\"\"\n",
        "    Handle text that may have encoding issues (latin-1, utf-8, etc.)\n",
        "    - Try to decode with different encodings\n",
        "    - Replace problematic characters with approximations\n",
        "    - Handle encoding errors gracefully\n",
        "\n",
        "    Args:\n",
        "        text: Potentially corrupted text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text with proper encoding\n",
        "    \"\"\"\n",
        "    # TODO: Handle encoding errors\n",
        "    # TODO: Replace problematic characters\n",
        "    # Hint: Use .encode() and .decode() with error handling\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie \"Inception\" was amazing! üé¨\n",
        "    It's a sci-fi masterpiece with great actors.\n",
        "    The director's name is Christopher Nolan.\n",
        "    Rating: 9/10 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Clean unicode and test\n",
        "# cleaned = clean_unicode_text(test_text)\n",
        "# print(\"Cleaned:\", cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwf8OiDHrJsc"
      },
      "outputs": [],
      "source": [
        "# Exercise 6b: Handle Inconsistent Capitalization\n",
        "\n",
        "def smart_lowercase(text, preserve_acronyms=True, preserve_proper_nouns=False):\n",
        "    \"\"\"\n",
        "    Convert text to lowercase intelligently:\n",
        "    - Option 1: Preserve acronyms (NASA, AI, USA ‚Üí keep uppercase)\n",
        "    - Option 2: Preserve proper nouns (names, places) ‚Üí more complex!\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        preserve_acronyms: If True, keep acronyms uppercase\n",
        "        preserve_proper_nouns: If True, try to preserve proper nouns (challenging!)\n",
        "\n",
        "    Returns:\n",
        "        str: Smartly lowercased text\n",
        "    \"\"\"\n",
        "    # TODO: If preserve_acronyms, detect acronyms (all caps, 2+ chars)\n",
        "    # TODO: Convert rest to lowercase\n",
        "    # TODO: If preserve_proper_nouns, use heuristics (capitalized words at sentence start)\n",
        "    # Note: Full proper noun detection requires NER (Named Entity Recognition) - not covered here!\n",
        "\n",
        "    return text\n",
        "\n",
        "def handle_title_case(text):\n",
        "    \"\"\"\n",
        "    Normalize title case inconsistencies.\n",
        "    - \"Star Wars\" vs \"STAR WARS\" vs \"star wars\" ‚Üí \"star wars\"\n",
        "    - But preserve intentional capitalization when needed\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    # TODO: Handle different capitalization styles\n",
        "    # TODO: Convert to consistent lowercase (or preserve known proper nouns)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie AI is about artificial intelligence.\n",
        "    NASA scientists worked on the film.\n",
        "    The director is Christopher Nolan, not CHRIS NOLAN.\n",
        "    The film STAR WARS is a classic.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test smart lowercase\n",
        "# smart_lower = smart_lowercase(test_text, preserve_acronyms=True)\n",
        "# print(\"Smart lowercase:\", smart_lower)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9JKFQusrJsc"
      },
      "outputs": [],
      "source": [
        "# Exercise 6c: Handle Missing and Corrupted Data\n",
        "\n",
        "def preprocess_robust(text):\n",
        "    \"\"\"\n",
        "    Robust preprocessing that handles:\n",
        "    - None/NaN values\n",
        "    - Empty strings\n",
        "    - Whitespace-only strings\n",
        "    - Very long strings (truncate if needed)\n",
        "    - Non-string types (convert to string)\n",
        "\n",
        "    Args:\n",
        "        text: Potentially problematic input\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text or empty string if invalid\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # TODO: Check if text is None or NaN\n",
        "    # TODO: Check if text is empty or whitespace-only\n",
        "    # TODO: Convert to string if not already\n",
        "    # TODO: Handle edge cases (too long, wrong type, etc.)\n",
        "\n",
        "    if text is None or (isinstance(text, float) and np.isnan(text)):\n",
        "        return \"\"\n",
        "\n",
        "    # TODO: Continue with cleaning...\n",
        "\n",
        "    return str(text) if text else \"\"\n",
        "\n",
        "def batch_preprocess_robust(texts):\n",
        "    \"\"\"\n",
        "    Preprocess a list of texts, handling missing/corrupted entries.\n",
        "\n",
        "    Args:\n",
        "        texts: List of texts (may contain None, NaN, etc.)\n",
        "\n",
        "    Returns:\n",
        "        list: List of cleaned texts (same length, invalid entries become empty strings)\n",
        "    \"\"\"\n",
        "    # TODO: Process each text with preprocess_robust\n",
        "    # TODO: Maintain same length as input\n",
        "    # TODO: Log or track which entries were invalid\n",
        "\n",
        "    cleaned = []\n",
        "    invalid_indices = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        cleaned_text = preprocess_robust(text)\n",
        "        if not cleaned_text:\n",
        "            invalid_indices.append(i)\n",
        "        cleaned.append(cleaned_text)\n",
        "\n",
        "    if invalid_indices:\n",
        "        print(f\"Warning: {len(invalid_indices)} invalid entries found at indices: {invalid_indices[:10]}...\")\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# Test cases\n",
        "test_texts = [\n",
        "    \"Normal text here\",\n",
        "    None,\n",
        "    \"\",\n",
        "    \"   \",  # whitespace only\n",
        "    \"Valid text with content\",\n",
        "    float('nan'),\n",
        "    \"Another valid entry\",\n",
        "    12345,  # number instead of string\n",
        "    \"Good text\"\n",
        "]\n",
        "\n",
        "# TODO: Test robust preprocessing\n",
        "# cleaned_texts = batch_preprocess_robust(test_texts)\n",
        "# print(\"Cleaned texts:\", cleaned_texts)\n",
        "# print(f\"Valid entries: {sum(1 for t in cleaned_texts if t)}/{len(cleaned_texts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYs8QvTBrJsc"
      },
      "outputs": [],
      "source": [
        "# Exercise 6d: Handle Numbers and Units in Text\n",
        "\n",
        "def normalize_numbers_and_units(text):\n",
        "    \"\"\"\n",
        "    Normalize numbers and units for better text processing:\n",
        "    - \"100 years\" ‚Üí \"100_years\" or \"[NUMBER] years\" (preserve context)\n",
        "    - \"50%\" ‚Üí \"50_percent\" or \"[PERCENTAGE]\"\n",
        "    - \"3.5 stars\" ‚Üí \"3.5_stars\" or \"[RATING]\"\n",
        "\n",
        "    Options:\n",
        "    1. Replace with placeholders: \"[NUMBER]\", \"[PERCENTAGE]\", etc.\n",
        "    2. Keep as-is but mark: \"100_years\" (replace space with underscore)\n",
        "    3. Remove entirely: \"100 years\" ‚Üí \"years\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with normalized numbers/units\n",
        "    \"\"\"\n",
        "    # TODO: Detect numbers with units (years, dollars, percent, etc.)\n",
        "    # TODO: Normalize format (choose one approach above)\n",
        "    # TODO: Handle different number formats (integers, decimals, percentages)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_numeric_metadata(text):\n",
        "    \"\"\"\n",
        "    Extract numeric metadata (ratings, years, amounts) and store separately.\n",
        "    This allows keeping text clean while preserving important numeric information.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'text': cleaned text (numbers removed or replaced),\n",
        "            'ratings': [list of ratings],\n",
        "            'years': [list of years],\n",
        "            'amounts': [list of monetary amounts],\n",
        "            'percentages': [list of percentages]\n",
        "        }\n",
        "    \"\"\"\n",
        "    # TODO: Extract different types of numbers\n",
        "    # TODO: Remove or replace them in text\n",
        "    # TODO: Return both cleaned text and extracted metadata\n",
        "\n",
        "    metadata = {\n",
        "        'text': text,\n",
        "        'ratings': [],\n",
        "        'years': [],\n",
        "        'amounts': [],\n",
        "        'percentages': []\n",
        "    }\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie was released in 2010 and grossed $800 million.\n",
        "    It has a rating of 8.7/10 with 95% positive reviews.\n",
        "    The runtime is 148 minutes and it won 4 Oscars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test number normalization\n",
        "# normalized = normalize_numbers_and_units(test_text)\n",
        "# metadata = extract_numeric_metadata(test_text)\n",
        "# print(\"Normalized:\", normalized)\n",
        "# print(\"Metadata:\", metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzA0E_9frJsd"
      },
      "outputs": [],
      "source": [
        "# Exercise 8e: Complete Robust Preprocessing Pipeline\n",
        "\n",
        "def preprocess_robust_pipeline(text,\n",
        "                               handle_unicode=True,\n",
        "                               handle_capitalization=True,\n",
        "                               handle_numbers=True,\n",
        "                               handle_missing=True):\n",
        "    \"\"\"\n",
        "    Complete robust preprocessing pipeline that handles all edge cases.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Handle missing/corrupted data\n",
        "    2. Handle unicode and special characters\n",
        "    3. Handle capitalization (smart lowercase)\n",
        "    4. Handle numbers and units (normalize or extract)\n",
        "    5. Basic cleaning (URLs, emails, etc. - from Exercise 1)\n",
        "    6. Normalize whitespace\n",
        "\n",
        "    Args:\n",
        "        text: Raw input text\n",
        "        handle_unicode: Whether to clean unicode\n",
        "        handle_capitalization: Whether to apply smart lowercase\n",
        "        handle_numbers: Whether to normalize numbers/units\n",
        "        handle_missing: Whether to handle missing data\n",
        "\n",
        "    Returns:\n",
        "        str: Fully preprocessed text\n",
        "    \"\"\"\n",
        "    # TODO: Step 1: Handle missing data\n",
        "    if handle_missing:\n",
        "        text = preprocess_robust(text)\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "    # TODO: Step 2: Handle unicode\n",
        "    if handle_unicode:\n",
        "        text = clean_unicode_text(text)\n",
        "\n",
        "    # TODO: Step 3: Handle capitalization\n",
        "    if handle_capitalization:\n",
        "        text = smart_lowercase(text, preserve_acronyms=True)\n",
        "\n",
        "    # TODO: Step 4: Handle numbers (optional - normalize or extract)\n",
        "    if handle_numbers:\n",
        "        # Option: Normalize or extract metadata\n",
        "        text = normalize_numbers_and_units(text)\n",
        "\n",
        "    # TODO: Step 5: Basic cleaning (from Exercise 1)\n",
        "    # text = clean_text(text)  # Use your function from Exercise 1\n",
        "\n",
        "    # TODO: Step 6: Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on real movie data\n",
        "print(\"Testing robust preprocessing on movie descriptions:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test on a few movie descriptions (handle potential missing data)\n",
        "for idx in range(min(5, len(df))):\n",
        "    original = df.loc[idx, 'description']\n",
        "    if pd.isna(original):\n",
        "        print(f\"\\nMovie {idx}: [MISSING DATA]\")\n",
        "        continue\n",
        "\n",
        "    processed = preprocess_robust_pipeline(original)\n",
        "\n",
        "    print(f\"\\nMovie {idx}:\")\n",
        "    print(f\"Original (first 100 chars): {str(original)[:100]}...\")\n",
        "    print(f\"Processed (first 100 chars): {processed[:100]}...\")\n",
        "    print(f\"Length: {len(str(original))} ‚Üí {len(processed)} chars\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üí° Key Insight: Robust preprocessing handles real-world data issues!\")\n",
        "print(\"   Always test your preprocessing on actual data to find edge cases.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}